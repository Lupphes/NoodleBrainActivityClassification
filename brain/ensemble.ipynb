{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba47be98",
   "metadata": {
    "papermill": {
     "duration": 0.008524,
     "end_time": "2024-03-07T00:25:29.256594",
     "exception": false,
     "start_time": "2024-03-07T00:25:29.248070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IIACT: Ensemble for HMS Brain Comp by MLiP team\n",
    "This is combination and ensemble notebook for Kaggle's HMS brain comp. based on [Previous Notebook][https://www.kaggle.com/code/luepoe/iiact-ensamble-features-head-starter].\n",
    "\n",
    " We choose these models to work \n",
    "- Kaggle's spectrograms (CV 0.6123 – LB 0.41)\n",
    "- Chris's EEG spectrograms(modified version) (CV 0.6288 – LB 0.39)\n",
    "- Both Kaggle and EEG spectrograms (CV 0.5646 – LB 0.37)\n",
    "- Chris's [WaveNet][4] (CV 0.6992 - LB 0.41)\n",
    "- HMS baseline_resnet34d(512*512 inference 6 models) [Notebook][https://www.kaggle.com/code/yunsuxiaozi/hms-baseline-resnet34d-512-512-inference-6-models/notebook]\n",
    "- EfficientNetB0 Starter - [LB 0.43]: [Notebook][https://www.kaggle.com/code/cdeotte/efficientnetb0-starter-lb-0-43]\n",
    "- CatBoost Starter - [LB 0.60]: [Notebook][https://www.kaggle.com/code/cdeotte/catboost-starter-lb-0-60]\n",
    "\n",
    "**The Ensemble achieves LB ????** \n",
    "\n",
    "Features+Head Starter uses Chris Deotte's Kaggle dataset [here][1]. Also Uses Chris's EEG spectrograms [here][3] (modified version) \n",
    "\n",
    "### Train and Infer Tips\n",
    "\n",
    "This notebook can be used both to train and submit (infer) to Kaggle LB. When training, you can set variable `submission = False` , you can also set `TEST_MODE = TRUE` to upload 500 samples queckly instead of the whole dataset for testing. \n",
    "\n",
    "To train a specific model type, you should set `DATA_TYPE = 'both|eeg|kaggle|raw'`, `kaggle` to train on Kaggle's spectrograms, `eeg` to train on EEG's spectrograms, `both` to train on Kaggle's and EEG's spectrograms, `raw` to train on EEG's signal with WaveNet,\n",
    "\n",
    "For submission after training models, you should save them in the `config.futures_head_starters_models` dataset, then run this notebook with `submission = True`.\n",
    "\n",
    "Once we have all the models saved to `config.futures_head_starters_models` and ready ensemble, we should set `submission = True` and `ENSEMBLE = True` and set the models versions that we prior specified, as well as their `LBs` for weighted ensemble.\n",
    "\n",
    "This notebook is made as generic as possible to expand and try different experiments.\n",
    "\n",
    "What you could do:\n",
    "- Change EfficientNetB(0-7) with `config.efficientnetb_tf_keras`\n",
    "- Data augmentation by setting DataGenerator's parameter to `augment = True`\n",
    "- Different image configurations as input.\n",
    "- WaveNet model tuning.\n",
    "\n",
    "\n",
    "This notebook is a direct descendent of Chris's notebook [here][2]\n",
    "\n",
    "[1]: https://www.kaggle.com/datasets/cdeotte/brain-spectrograms\n",
    "[2]: https://www.kaggle.com/code/cdeotte/efficientnetb2-starter-lb-0-57\n",
    "[3]: https://www.kaggle.com/datasets/nartaa/eeg-spectrograms\n",
    "[4]: https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/468684\n",
    "[5]: https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/477461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install d2l --no-index --find-links=file:///kaggle/input/d2l-package/d2l/\n",
    "# !pip install /kaggle/input/brain-solver/brain_solver-0.9.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcd4a036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 GPU\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# Data manipulation and numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "\n",
    "# Audio and signal processing libraries\n",
    "import librosa\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Image augmentation library\n",
    "import albumentations as albu\n",
    "\n",
    "# Machine Learning and Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Multiply, Add, Conv1D, Concatenate\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "# Visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom module imports\n",
    "from brain_solver import Helpers as hp, Trainer as tr, BrainModel as br, EEGDataset\n",
    "\n",
    "# Suppress warnings if desired\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seed setting for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Setup for CUDA device selection\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TensorFlow GPU configuration and strategy setup\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "if len(gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f'Using {len(gpus)} GPUs')\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    print(f'Using {len(gpus)} GPU')\n",
    "\n",
    "# TensorFlow GPU configuration for memory growth (optional, uncomment if needed)\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         # Currently, memory growth needs to be the same across GPUs\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         # Memory growth must be set before GPUs have been initialized\n",
    "#         print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d2630",
   "metadata": {},
   "source": [
    "# Config Class Summary\n",
    "\n",
    "The `Config` class manages configurations for a brain activity classification project. It includes:\n",
    "\n",
    "- **Data and Model Paths**: Centralizes paths for data (e.g., EEG, spectrograms) and model checkpoints.\n",
    "- **Training Parameters**: Configures training details like epochs, batch size, and learning rate.\n",
    "- **Feature Flags**: Toggles for using wavelets, spectrograms, and reading options.\n",
    "\n",
    "Designed for easy adjustments to facilitate model development and experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b69a9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brain_solver import Config\n",
    "full_path = \"/home/osloup/NoodleNappers/brain/data/\"\n",
    "# full_path = \"C:/Users/tygof/Documents/Semester 8/MLiP/NoodleNappers/brain/data/\"\n",
    "config = Config(full_path,  full_path + \"out/\", USE_EEG_SPECTROGRAMS=True, USE_KAGGLE_SPECTROGRAMS=True, should_read_brain_spectograms=False, should_read_eeg_spectrogram_files=False, USE_PRETRAINED_MODEL=False)\n",
    "\n",
    "# full_path = \"/kaggle/input/\"\n",
    "# config = Config(full_path, \"/kaggle/working/\", USE_EEG_SPECTROGRAMS=True, USE_KAGGLE_SPECTROGRAMS=True, should_read_brain_spectograms=False, should_read_eeg_spectrogram_files=False, USE_PRETRAINED_MODEL=False)\n",
    "\n",
    "import sys\n",
    "sys.path.append(full_path + 'kaggle-kl-div')\n",
    "from kaggle_kl_div import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f013eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = 'both' # both|eeg|kaggle|raw\n",
    "TEST_MODE = True\n",
    "submission = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd2964d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:29.274122Z",
     "iopub.status.busy": "2024-03-07T00:25:29.273761Z",
     "iopub.status.idle": "2024-03-07T00:25:45.717819Z",
     "shell.execute_reply": "2024-03-07T00:25:45.716662Z"
    },
    "papermill": {
     "duration": 16.455098,
     "end_time": "2024-03-07T00:25:45.720022",
     "exception": false,
     "start_time": "2024-03-07T00:25:29.264924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 47\n",
    "# Setup for ensemble\n",
    "ENSEMBLE = True\n",
    "LBs = [0.37,0.39,0.41,0.41] # for weighted ensemble we use LBs of each model\n",
    "VERK = 43 # Kaggle's spectrogram model version\n",
    "VERB = 47 # Kaggle's and EEG's spectrogram model version\n",
    "VERE = 42 # EEG's spectrogram model version\n",
    "VERR = 37 # EEG's raw wavenet model version, trained on single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b5c2d1",
   "metadata": {
    "papermill": {
     "duration": 0.009237,
     "end_time": "2024-03-07T00:25:45.738970",
     "exception": false,
     "start_time": "2024-03-07T00:25:45.729733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and create Non-Overlapping Eeg Id Train Data\n",
    "The competition data description says that test data does not have multiple crops from the same `eeg_id`. Therefore we will train and validate using only 1 crop per `eeg_id`. There is a discussion about this [here][1].\n",
    "[1]: https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/467021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af49961f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:45.759527Z",
     "iopub.status.busy": "2024-03-07T00:25:45.758910Z",
     "iopub.status.idle": "2024-03-07T00:25:45.774327Z",
     "shell.execute_reply": "2024-03-07T00:25:45.773312Z"
    },
    "papermill": {
     "duration": 0.028356,
     "end_time": "2024-03-07T00:25:45.776740",
     "exception": false,
     "start_time": "2024-03-07T00:25:45.748384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "FEATS2 = ['Fp1','T3','C3','O1','Fp2','C4','T4','O2']\n",
    "FEAT2IDX = {x:y for x,y in zip(FEATS2,range(len(FEATS2)))}\n",
    "\n",
    "def eeg_from_parquet(parquet_path):\n",
    "\n",
    "    eeg = pd.read_parquet(parquet_path, columns=FEATS2)\n",
    "    rows = len(eeg)\n",
    "    offset = (rows-10_000)//2\n",
    "    eeg = eeg.iloc[offset:offset+10_000]\n",
    "    data = np.zeros((10_000,len(FEATS2)))\n",
    "    for j,col in enumerate(FEATS2):\n",
    "        \n",
    "        # FILL NAN\n",
    "        x = eeg[col].values.astype('float32')\n",
    "        m = np.nanmean(x)\n",
    "        if np.isnan(x).mean()<1: x = np.nan_to_num(x,nan=m)\n",
    "        else: x[:] = 0\n",
    "        \n",
    "        data[:,j] = x\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_kl(data):\n",
    "    import torch\n",
    "    labels = data[TARGETS].values + 1e-5\n",
    "\n",
    "    # compute kl-loss with uniform distribution by pytorch\n",
    "    data['kl'] = torch.nn.functional.kl_div(\n",
    "        torch.log(torch.tensor(labels)),\n",
    "        torch.tensor([1 / 6] * 6),\n",
    "        reduction='none'\n",
    "    ).sum(dim=1).numpy()\n",
    "    return data\n",
    "\n",
    "def reset_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "if not submission:\n",
    "    train = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\n",
    "    TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "    META = ['spectrogram_id','spectrogram_label_offset_seconds','patient_id','expert_consensus']\n",
    "    train = train.groupby('eeg_id')[META+TARGETS\n",
    "                           ].agg({**{m:'first' for m in META},**{t:'sum' for t in TARGETS}}).reset_index() \n",
    "    train[TARGETS] = train[TARGETS]/train[TARGETS].values.sum(axis=1,keepdims=True)\n",
    "    train.columns = ['eeg_id','spec_id','offset','patient_id','target'] + TARGETS\n",
    "    train = add_kl(train)\n",
    "    print(train.head(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be80f6",
   "metadata": {
    "papermill": {
     "duration": 0.00905,
     "end_time": "2024-03-07T00:25:45.795384",
     "exception": false,
     "start_time": "2024-03-07T00:25:45.786334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read Train Spectrograms and EEGs\n",
    "\n",
    "We can read 3 file from Chris's [Kaggle dataset here][1] which contains all the 11k spectrograms. From Chris's modified EEG spectrogram [here][2]. From Chris's EEG signals [here][3]\n",
    "\n",
    "[1]: https://www.kaggle.com/datasets/cdeotte/brain-spectrograms\n",
    "[2]: https://www.kaggle.com/datasets/nartaa/eeg-spectrograms\n",
    "[3]: https://www.kaggle.com/datasets/cdeotte/brain-eegs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf0bb9d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:45.815721Z",
     "iopub.status.busy": "2024-03-07T00:25:45.814915Z",
     "iopub.status.idle": "2024-03-07T00:25:45.826741Z",
     "shell.execute_reply": "2024-03-07T00:25:45.825681Z"
    },
    "papermill": {
     "duration": 0.024794,
     "end_time": "2024-03-07T00:25:45.829355",
     "exception": false,
     "start_time": "2024-03-07T00:25:45.804561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not submission:\n",
    "    # FOR TESTING SET TEST_MODE TO TRUE\n",
    "    if TEST_MODE:\n",
    "        train = train.sample(500,random_state=42).reset_index(drop=True)\n",
    "        spectrograms = {}\n",
    "        for i,e in enumerate(train.spec_id.values):\n",
    "            if i%100==0: print(i,', ',end='')\n",
    "            x = pd.read_parquet(f'{config.data_spectograms}{e}.parquet')\n",
    "            spectrograms[e] = x.values\n",
    "        all_eegs = {}\n",
    "        for i,e in enumerate(train.eeg_id.values):\n",
    "            if i%100==0: print(i,', ',end='')\n",
    "            x = np.load(f'{config.path_to_eeg_spectrograms_folder}{e}.npy')\n",
    "            all_eegs[e] = x\n",
    "        all_raw_eegs = {}\n",
    "        for i,e in enumerate(train.eeg_id.values):\n",
    "            if i%100==0: print(i,', ',end='')\n",
    "            x = eeg_from_parquet(f'{config.data_eeg}{e}.parquet')              \n",
    "            all_raw_eegs[e] = x\n",
    "    else:\n",
    "        spectrograms = None\n",
    "        all_eegs = None\n",
    "        all_raw_eegs = None\n",
    "        if DATA_TYPE=='both' or DATA_TYPE=='kaggle':\n",
    "            spectrograms = np.load(config.path_to_brain_spectrograms_npy,allow_pickle=True).item()\n",
    "        if DATA_TYPE=='both' or DATA_TYPE=='eeg':\n",
    "            all_eegs = np.load(config.path_to_eeg_spectrograms_npy,allow_pickle=True).item()\n",
    "        if DATA_TYPE=='raw':\n",
    "            all_raw_eegs = np.load(config.brain_eegs_npy,allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa86588",
   "metadata": {
    "papermill": {
     "duration": 0.008963,
     "end_time": "2024-03-07T00:25:45.847743",
     "exception": false,
     "start_time": "2024-03-07T00:25:45.838780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA GENERATOR\n",
    "This data generator outputs 512x512x3, the spectrogram and eeg images are concatenated all togother in a single image. For using data augmention you can set `augment = True` when creating the train data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05f27190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:45.868672Z",
     "iopub.status.busy": "2024-03-07T00:25:45.868141Z",
     "iopub.status.idle": "2024-03-07T00:25:48.412022Z",
     "shell.execute_reply": "2024-03-07T00:25:48.411197Z"
    },
    "papermill": {
     "duration": 2.557436,
     "end_time": "2024-03-07T00:25:48.414438",
     "exception": false,
     "start_time": "2024-03-07T00:25:45.857002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data, specs=None, eeg_specs=None, raw_eegs=None, augment=False, mode='train', data_type=DATA_TYPE): \n",
    "        self.data = data\n",
    "        self.augment = augment\n",
    "        self.mode = mode\n",
    "        self.data_type = data_type\n",
    "        self.specs = specs\n",
    "        self.eeg_specs = eeg_specs\n",
    "        self.raw_eegs = raw_eegs\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.data_generation(index)\n",
    "        if self.augment: X = self.augmentation(X)\n",
    "        return X, y\n",
    "    \n",
    "    def __call__(self):\n",
    "        for i in range(self.__len__()):\n",
    "            yield self.__getitem__(i)\n",
    "            \n",
    "            if i == self.__len__()-1:\n",
    "                self.on_epoch_end()\n",
    "                \n",
    "    def on_epoch_end(self):\n",
    "        if self.mode=='train': \n",
    "            self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def data_generation(self, index):\n",
    "        if self.data_type == 'both':\n",
    "            X,y = self.generate_all_specs(index)\n",
    "        elif self.data_type == 'eeg' or self.data_type == 'kaggle':\n",
    "            X,y = self.generate_specs(index)\n",
    "        elif self.data_type == 'raw':\n",
    "            X,y = self.generate_raw(index)\n",
    "\n",
    "        return X,y\n",
    "    \n",
    "    def generate_all_specs(self, index):\n",
    "        X = np.zeros((512,512,3),dtype='float32')\n",
    "        y = np.zeros((6,),dtype='float32')\n",
    "        \n",
    "        row = self.data.iloc[index]\n",
    "        if self.mode=='test': \n",
    "            offset = 0\n",
    "        else:\n",
    "            offset = int(row.offset/2)\n",
    "            \n",
    "        eeg = self.eeg_specs[row.eeg_id]\n",
    "        spec = self.specs[row.spec_id]\n",
    "        \n",
    "        imgs = [spec[offset:offset+300,k*100:(k+1)*100].T for k in [0,2,1,3]] # to match kaggle with eeg\n",
    "        img = np.stack(imgs,axis=-1)\n",
    "        # LOG TRANSFORM SPECTROGRAM\n",
    "        img = np.clip(img,np.exp(-4),np.exp(8))\n",
    "        img = np.log(img)\n",
    "            \n",
    "        # STANDARDIZE PER IMAGE\n",
    "        img = np.nan_to_num(img, nan=0.0)    \n",
    "            \n",
    "        mn = img.flatten().min()\n",
    "        mx = img.flatten().max()\n",
    "        ep = 1e-5\n",
    "        img = 255 * (img - mn) / (mx - mn + ep)\n",
    "        \n",
    "        X[0_0+56:100+56,:256,0] = img[:,22:-22,0] # LL_k\n",
    "        X[100+56:200+56,:256,0] = img[:,22:-22,2] # RL_k\n",
    "        X[0_0+56:100+56,:256,1] = img[:,22:-22,1] # LP_k\n",
    "        X[100+56:200+56,:256,1] = img[:,22:-22,3] # RP_k\n",
    "        X[0_0+56:100+56,:256,2] = img[:,22:-22,2] # RL_k\n",
    "        X[100+56:200+56,:256,2] = img[:,22:-22,1] # LP_k\n",
    "        \n",
    "        X[0_0+56:100+56,256:,0] = img[:,22:-22,0] # LL_k\n",
    "        X[100+56:200+56,256:,0] = img[:,22:-22,2] # RL_k\n",
    "        X[0_0+56:100+56,256:,1] = img[:,22:-22,1] # LP_k\n",
    "        X[100+56:200+56,256:,1] = img[:,22:-22,3] # RP_K\n",
    "        \n",
    "        # EEG\n",
    "        img = eeg\n",
    "        mn = img.flatten().min()\n",
    "        mx = img.flatten().max()\n",
    "        ep = 1e-5\n",
    "        img = 255 * (img - mn) / (mx - mn + ep)\n",
    "        X[200+56:300+56,:256,0] = img[:,22:-22,0] # LL_e\n",
    "        X[300+56:400+56,:256,0] = img[:,22:-22,2] # RL_e\n",
    "        X[200+56:300+56,:256,1] = img[:,22:-22,1] # LP_e\n",
    "        X[300+56:400+56,:256,1] = img[:,22:-22,3] # RP_e\n",
    "        X[200+56:300+56,:256,2] = img[:,22:-22,2] # RL_e\n",
    "        X[300+56:400+56,:256,2] = img[:,22:-22,1] # LP_e\n",
    "        \n",
    "        X[200+56:300+56,256:,0] = img[:,22:-22,0] # LL_e\n",
    "        X[300+56:400+56,256:,0] = img[:,22:-22,2] # RL_e\n",
    "        X[200+56:300+56,256:,1] = img[:,22:-22,1] # LP_e\n",
    "        X[300+56:400+56,256:,1] = img[:,22:-22,3] # RP_e\n",
    "\n",
    "        if self.mode!='test':\n",
    "            y[:] = row[TARGETS]\n",
    "        \n",
    "        return X,y\n",
    "    \n",
    "    def generate_specs(self, index):\n",
    "        X = np.zeros((512,512,3),dtype='float32')\n",
    "        y = np.zeros((6,),dtype='float32')\n",
    "        \n",
    "        row = self.data.iloc[index]\n",
    "        if self.mode=='test': \n",
    "            offset = 0\n",
    "        else:\n",
    "            offset = int(row.offset/2)\n",
    "            \n",
    "        if self.data_type == 'eeg':\n",
    "            img = self.eeg_specs[row.eeg_id]\n",
    "        elif self.data_type == 'kaggle':\n",
    "            spec = self.specs[row.spec_id]\n",
    "            imgs = [spec[offset:offset+300,k*100:(k+1)*100].T for k in [0,2,1,3]] # to match kaggle with eeg\n",
    "            img = np.stack(imgs,axis=-1)\n",
    "            # LOG TRANSFORM SPECTROGRAM\n",
    "            img = np.clip(img,np.exp(-4),np.exp(8))\n",
    "            img = np.log(img)\n",
    "            \n",
    "            # STANDARDIZE PER IMAGE\n",
    "            img = np.nan_to_num(img, nan=0.0)    \n",
    "            \n",
    "        mn = img.flatten().min()\n",
    "        mx = img.flatten().max()\n",
    "        ep = 1e-5\n",
    "        img = 255 * (img - mn) / (mx - mn + ep)\n",
    "        \n",
    "        X[0_0+56:100+56,:256,0] = img[:,22:-22,0]\n",
    "        X[100+56:200+56,:256,0] = img[:,22:-22,2]\n",
    "        X[0_0+56:100+56,:256,1] = img[:,22:-22,1]\n",
    "        X[100+56:200+56,:256,1] = img[:,22:-22,3]\n",
    "        X[0_0+56:100+56,:256,2] = img[:,22:-22,2]\n",
    "        X[100+56:200+56,:256,2] = img[:,22:-22,1]\n",
    "        \n",
    "        X[0_0+56:100+56,256:,0] = img[:,22:-22,0]\n",
    "        X[100+56:200+56,256:,0] = img[:,22:-22,1]\n",
    "        X[0_0+56:100+56,256:,1] = img[:,22:-22,2]\n",
    "        X[100+56:200+56,256:,1] = img[:,22:-22,3]\n",
    "        \n",
    "        X[200+56:300+56,:256,0] = img[:,22:-22,0]\n",
    "        X[300+56:400+56,:256,0] = img[:,22:-22,1]\n",
    "        X[200+56:300+56,:256,1] = img[:,22:-22,2]\n",
    "        X[300+56:400+56,:256,1] = img[:,22:-22,3]\n",
    "        X[200+56:300+56,:256,2] = img[:,22:-22,3]\n",
    "        X[300+56:400+56,:256,2] = img[:,22:-22,2]\n",
    "        \n",
    "        X[200+56:300+56,256:,0] = img[:,22:-22,0]\n",
    "        X[300+56:400+56,256:,0] = img[:,22:-22,2]\n",
    "        X[200+56:300+56,256:,1] = img[:,22:-22,1]\n",
    "        X[300+56:400+56,256:,1] = img[:,22:-22,3]\n",
    "        \n",
    "        if self.mode!='test':\n",
    "            y[:] = row[TARGETS]\n",
    "        \n",
    "        return X,y\n",
    "    \n",
    "    def generate_raw(self,index):\n",
    "        X = np.zeros((10_000,8),dtype='float32')\n",
    "        y = np.zeros((6,),dtype='float32')\n",
    "        \n",
    "        row = self.data.iloc[index]\n",
    "        eeg = self.raw_eegs[row.eeg_id]\n",
    "            \n",
    "        # FEATURE ENGINEER\n",
    "        X[:,0] = eeg[:,FEAT2IDX['Fp1']] - eeg[:,FEAT2IDX['T3']]\n",
    "        X[:,1] = eeg[:,FEAT2IDX['T3']] - eeg[:,FEAT2IDX['O1']]\n",
    "            \n",
    "        X[:,2] = eeg[:,FEAT2IDX['Fp1']] - eeg[:,FEAT2IDX['C3']]\n",
    "        X[:,3] = eeg[:,FEAT2IDX['C3']] - eeg[:,FEAT2IDX['O1']]\n",
    "            \n",
    "        X[:,4] = eeg[:,FEAT2IDX['Fp2']] - eeg[:,FEAT2IDX['C4']]\n",
    "        X[:,5] = eeg[:,FEAT2IDX['C4']] - eeg[:,FEAT2IDX['O2']]\n",
    "            \n",
    "        X[:,6] = eeg[:,FEAT2IDX['Fp2']] - eeg[:,FEAT2IDX['T4']]\n",
    "        X[:,7] = eeg[:,FEAT2IDX['T4']] - eeg[:,FEAT2IDX['O2']]\n",
    "            \n",
    "        # STANDARDIZE\n",
    "        X = np.clip(X,-1024,1024)\n",
    "        X = np.nan_to_num(X, nan=0) / 32.0\n",
    "            \n",
    "        # BUTTER LOW-PASS FILTER\n",
    "        X = self.butter_lowpass_filter(X)\n",
    "        # Downsample\n",
    "        X = X[::5,:]\n",
    "        \n",
    "        if self.mode!='test':\n",
    "            y[:] = row[TARGETS]\n",
    "                \n",
    "        return X,y\n",
    "        \n",
    "    def butter_lowpass_filter(self, data, cutoff_freq=20, sampling_rate=200, order=4):\n",
    "        nyquist = 0.5 * sampling_rate\n",
    "        normal_cutoff = cutoff_freq / nyquist\n",
    "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "        filtered_data = lfilter(b, a, data, axis=0)\n",
    "        return filtered_data\n",
    "    \n",
    "    def resize(self, img,size):\n",
    "        composition = albu.Compose([\n",
    "                albu.Resize(size[0],size[1])\n",
    "            ])\n",
    "        return composition(image=img)['image']\n",
    "            \n",
    "    def augmentation(self, img):\n",
    "        composition = albu.Compose([\n",
    "                albu.HorizontalFlip(p=0.4)\n",
    "            ])\n",
    "        return composition(image=img)['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b193853a",
   "metadata": {
    "papermill": {
     "duration": 0.008674,
     "end_time": "2024-03-07T00:25:48.431914",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.423240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DISPLAY DATA GENERATOR\n",
    "Below we display example data generator spectrogram images and raw EEG signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6eebdac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:48.451257Z",
     "iopub.status.busy": "2024-03-07T00:25:48.450313Z",
     "iopub.status.idle": "2024-03-07T00:25:48.460159Z",
     "shell.execute_reply": "2024-03-07T00:25:48.459171Z"
    },
    "papermill": {
     "duration": 0.021852,
     "end_time": "2024-03-07T00:25:48.462241",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.440389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not submission and DATA_TYPE!='raw':\n",
    "    gen = DataGenerator(train, augment=False, specs=spectrograms, eeg_specs=all_eegs, data_type=DATA_TYPE)\n",
    "    for x,y in gen:\n",
    "        break\n",
    "    plt.imshow(x[:,:,0])\n",
    "    plt.title(f'Target = {y.round(1)}',size=12)\n",
    "    plt.yticks([])\n",
    "    plt.ylabel('Frequencies (Hz)',size=12)\n",
    "    plt.xlabel('Time (sec)',size=12)\n",
    "    plt.show()\n",
    "    \n",
    "if not submission and DATA_TYPE=='raw':\n",
    "    gen = DataGenerator(train, raw_eegs=all_raw_eegs, data_type=DATA_TYPE)\n",
    "    for x,y in gen:\n",
    "        plt.figure(figsize=(20,4))\n",
    "        offset = 0\n",
    "        for j in range(x.shape[-1]):\n",
    "            if j!=0: offset -= x[:,j].min()\n",
    "            plt.plot(range(2_000),x[:,j]+offset,label=f'feature {j+1}')\n",
    "            offset += x[:,j].max()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6776c40",
   "metadata": {
    "papermill": {
     "duration": 0.009701,
     "end_time": "2024-03-07T00:25:48.480913",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.471212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008efa1",
   "metadata": {
    "papermill": {
     "duration": 0.008451,
     "end_time": "2024-03-07T00:25:48.498592",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.490141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LEARNING RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13c3d19b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:48.518315Z",
     "iopub.status.busy": "2024-03-07T00:25:48.517951Z",
     "iopub.status.idle": "2024-03-07T00:25:48.524207Z",
     "shell.execute_reply": "2024-03-07T00:25:48.523298Z"
    },
    "papermill": {
     "duration": 0.018704,
     "end_time": "2024-03-07T00:25:48.526412",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.507708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not submission:\n",
    "    def lrfn(epoch):\n",
    "        e3 = 1e-3 if DATA_TYPE=='raw' else 1e-4\n",
    "        return [1e-3,1e-3,e3,1e-4,1e-5][epoch]\n",
    "\n",
    "    LR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    \n",
    "    def lrfn2(epoch):\n",
    "        return [1e-5,1e-5,1e-6][epoch]\n",
    "\n",
    "    LR2 = tf.keras.callbacks.LearningRateScheduler(lrfn2, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe53691",
   "metadata": {
    "papermill": {
     "duration": 0.008455,
     "end_time": "2024-03-07T00:25:48.543512",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.535057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MODEL AND UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1b16cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:48.563038Z",
     "iopub.status.busy": "2024-03-07T00:25:48.562711Z",
     "iopub.status.idle": "2024-03-07T00:25:48.589026Z",
     "shell.execute_reply": "2024-03-07T00:25:48.588261Z"
    },
    "papermill": {
     "duration": 0.038748,
     "end_time": "2024-03-07T00:25:48.591018",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.552270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model():  \n",
    "    inp = tf.keras.layers.Input((512,512,3))\n",
    "    base_model = load_model(f'{config.efficientnetb_tf_keras}')    \n",
    "    x = base_model(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    output = tf.keras.layers.Dense(6,activation='softmax', dtype='float32')(x)\n",
    "    model = tf.keras.Model(inputs=inp, outputs=output)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "    loss = tf.keras.losses.KLDivergence()\n",
    "    model.compile(loss=loss, optimizer=opt)  \n",
    "    return model\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    kl = tf.keras.metrics.KLDivergence()\n",
    "    return kl(y_true, y_pred)\n",
    "\n",
    "def wave_block(x, filters, kernel_size, n):\n",
    "    dilation_rates = [2**i for i in range(n)]\n",
    "    x = Conv1D(filters = filters,\n",
    "               kernel_size = 1,\n",
    "               padding = 'same')(x)\n",
    "    res_x = x\n",
    "    for dilation_rate in dilation_rates:\n",
    "        tanh_out = Conv1D(filters = filters,\n",
    "                          kernel_size = kernel_size,\n",
    "                          padding = 'same', \n",
    "                          activation = 'tanh', \n",
    "                          dilation_rate = dilation_rate)(x)\n",
    "        sigm_out = Conv1D(filters = filters,\n",
    "                          kernel_size = kernel_size,\n",
    "                          padding = 'same',\n",
    "                          activation = 'sigmoid', \n",
    "                          dilation_rate = dilation_rate)(x)\n",
    "        x = Multiply()([tanh_out, sigm_out])\n",
    "        x = Conv1D(filters = filters,\n",
    "                   kernel_size = 1,\n",
    "                   padding = 'same')(x)\n",
    "        res_x = Add()([res_x, x])\n",
    "    return res_x\n",
    "\n",
    "def build_wave_model():\n",
    "        \n",
    "    # INPUT \n",
    "    inp = tf.keras.Input(shape=(2_000,8))\n",
    "    \n",
    "    ############\n",
    "    # FEATURE EXTRACTION SUB MODEL\n",
    "    inp2 = tf.keras.Input(shape=(2_000,1))\n",
    "    x = wave_block(inp2, 8, 4, 6)\n",
    "    x = wave_block(x, 16, 4, 6)\n",
    "    x = wave_block(x, 32, 4, 6)\n",
    "    x = wave_block(x, 64, 4, 6)\n",
    "    model2 = tf.keras.Model(inputs=inp2, outputs=x)\n",
    "    ###########\n",
    "    \n",
    "    # LEFT TEMPORAL CHAIN\n",
    "    x1 = model2(inp[:,:,0:1])\n",
    "    x1 = tf.keras.layers.GlobalAveragePooling1D()(x1)\n",
    "    x2 = model2(inp[:,:,1:2])\n",
    "    x2 = tf.keras.layers.GlobalAveragePooling1D()(x2)\n",
    "    z1 = tf.keras.layers.Average()([x1,x2])\n",
    "    \n",
    "    # LEFT PARASAGITTAL CHAIN\n",
    "    x1 = model2(inp[:,:,2:3])\n",
    "    x1 = tf.keras.layers.GlobalAveragePooling1D()(x1)\n",
    "    x2 = model2(inp[:,:,3:4])\n",
    "    x2 = tf.keras.layers.GlobalAveragePooling1D()(x2)\n",
    "    z2 = tf.keras.layers.Average()([x1,x2])\n",
    "    \n",
    "    # RIGHT PARASAGITTAL CHAIN\n",
    "    x1 = model2(inp[:,:,4:5])\n",
    "    x1 = tf.keras.layers.GlobalAveragePooling1D()(x1)\n",
    "    x2 = model2(inp[:,:,5:6])\n",
    "    x2 = tf.keras.layers.GlobalAveragePooling1D()(x2)\n",
    "    z3 = tf.keras.layers.Average()([x1,x2])\n",
    "    \n",
    "    # RIGHT TEMPORAL CHAIN\n",
    "    x1 = model2(inp[:,:,6:7])\n",
    "    x1 = tf.keras.layers.GlobalAveragePooling1D()(x1)\n",
    "    x2 = model2(inp[:,:,7:8])\n",
    "    x2 = tf.keras.layers.GlobalAveragePooling1D()(x2)\n",
    "    z4 = tf.keras.layers.Average()([x1,x2])\n",
    "    \n",
    "    # COMBINE CHAINS\n",
    "    y = tf.keras.layers.Concatenate()([z1,z2,z3,z4])\n",
    "    y = tf.keras.layers.Dense(64, activation='relu')(y)\n",
    "    y = tf.keras.layers.Dense(6,activation='softmax', dtype='float32')(y)\n",
    "    \n",
    "    # COMPILE MODEL\n",
    "    model = tf.keras.Model(inputs=inp, outputs=y)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "    loss = tf.keras.losses.KLDivergence()\n",
    "    model.compile(loss=loss, optimizer = opt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_hist(hist):\n",
    "    metrics = ['loss']\n",
    "    for i,metric in enumerate(metrics):\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(1,2,i+1)\n",
    "        plt.plot(hist[metric])\n",
    "        plt.plot(hist[f'val_{metric}'])\n",
    "        plt.title(f'{metric}',size=12)\n",
    "        plt.ylabel(f'{metric}',size=12)\n",
    "        plt.xlabel('epoch',size=12)\n",
    "        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f578c92",
   "metadata": {
    "papermill": {
     "duration": 0.008059,
     "end_time": "2024-03-07T00:25:48.607511",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.599452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adf6ebd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:48.625256Z",
     "iopub.status.busy": "2024-03-07T00:25:48.624908Z",
     "iopub.status.idle": "2024-03-07T00:25:48.643317Z",
     "shell.execute_reply": "2024-03-07T00:25:48.642431Z"
    },
    "papermill": {
     "duration": 0.029497,
     "end_time": "2024-03-07T00:25:48.645338",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.615841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not submission:\n",
    "    # for CV scores setting random seed works for single GPU only\n",
    "    reset_seed(42)\n",
    "    all_oof = []\n",
    "    all_true = []\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    total_hist = {}\n",
    "\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    for i, (train_index, valid_index) in enumerate(gkf.split(train, train.target, train.patient_id)):   \n",
    "        \n",
    "        print('#'*25)\n",
    "        print(f'### Fold {i+1}')\n",
    "        \n",
    "        data, val = train.iloc[train_index],train.iloc[valid_index]\n",
    "        train_gen = DataGenerator(data, augment=False, specs=spectrograms, eeg_specs=all_eegs, raw_eegs=all_raw_eegs)\n",
    "        valid_gen = DataGenerator(val, mode='valid', specs=spectrograms, eeg_specs=all_eegs, raw_eegs=all_raw_eegs)\n",
    "        data = data[data['kl']<5.5]\n",
    "        train_gen2 = DataGenerator(data, augment=False, specs=spectrograms, eeg_specs=all_eegs, raw_eegs=all_raw_eegs)\n",
    "        in_shape = (2000,8) if DATA_TYPE=='raw' else (512,512,3)\n",
    "        EPOCHS = 5\n",
    "        BATCH_SIZE_PER_REPLICA = 8\n",
    "        BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_generator(generator=train_gen, \n",
    "                                                   output_signature=(tf.TensorSpec(shape=in_shape, dtype=tf.float32),\n",
    "                                                                     tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        val_dataset = tf.data.Dataset.from_generator(generator=valid_gen, \n",
    "                                                   output_signature=(tf.TensorSpec(shape=in_shape, dtype=tf.float32),\n",
    "                                                                     tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        train_dataset2 = tf.data.Dataset.from_generator(generator=train_gen2, \n",
    "                                                   output_signature=(tf.TensorSpec(shape=in_shape, dtype=tf.float32),\n",
    "                                                                     tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "            \n",
    "        print(f'### train size {len(train_index)}, valid size {len(valid_index)}')\n",
    "        print('#'*25)\n",
    "        \n",
    "        K.clear_session()\n",
    "        with strategy.scope():\n",
    "            if DATA_TYPE=='raw':\n",
    "                model = build_wave_model()\n",
    "            else:\n",
    "                model = build_model()\n",
    "        \n",
    "        hist = model.fit(train_dataset, validation_data = val_dataset, \n",
    "                         epochs=EPOCHS, callbacks=[LR])\n",
    "        print(f'### seconds stage train size {len(data)}, valid size {len(val)}')\n",
    "        print('#'*25)\n",
    "        hist2 = model.fit(train_dataset2, validation_data = val_dataset, \n",
    "                         epochs=3, callbacks=[LR2])\n",
    "        losses.append(hist.history['loss']+hist2.history['loss'])\n",
    "        val_losses.append(hist.history['val_loss']+hist2.history['val_loss'])\n",
    "        with strategy.scope():\n",
    "            model.save_weights(f'model_{DATA_TYPE}_{VER}_{i}.weights.h5')\n",
    "        oof = model.predict(val_dataset, verbose=1)\n",
    "        all_oof.append(oof)\n",
    "        all_true.append(train.iloc[valid_index][TARGETS].values)    \n",
    "        del model, oof\n",
    "        gc.collect()\n",
    "        \n",
    "    total_hist['loss'] = np.mean(losses,axis=0)\n",
    "    total_hist['val_loss'] = np.mean(val_losses,axis=0)\n",
    "    all_oof = np.concatenate(all_oof)\n",
    "    all_true = np.concatenate(all_true)\n",
    "    plot_hist(total_hist)\n",
    "    print('#'*25)\n",
    "    print(f'CV KL SCORE: {score(all_true,all_oof)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac62e8a",
   "metadata": {
    "papermill": {
     "duration": 0.008326,
     "end_time": "2024-03-07T00:25:48.662244",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.653918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer Test and Create Submission CSV\n",
    "Infer the test data and create a `submission.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "851e3cbf",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:48.680058Z",
     "iopub.status.busy": "2024-03-07T00:25:48.679750Z",
     "iopub.status.idle": "2024-03-07T00:25:48.703031Z",
     "shell.execute_reply": "2024-03-07T00:25:48.702156Z"
    },
    "papermill": {
     "duration": 0.034482,
     "end_time": "2024-03-07T00:25:48.705011",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.670529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_WAVELET = None \n",
    "\n",
    "NAMES = ['LL','LP','RP','RR']\n",
    "\n",
    "FEATS = [['Fp1','F7','T3','T5','O1'],\n",
    "         ['Fp1','F3','C3','P3','O1'],\n",
    "         ['Fp2','F8','T4','T6','O2'],\n",
    "         ['Fp2','F4','C4','P4','O2']]\n",
    "\n",
    "# DENOISE FUNCTION\n",
    "def maddest(d, axis=None):\n",
    "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n",
    "\n",
    "def denoise(x, wavelet='haar', level=1):    \n",
    "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
    "    sigma = (1/0.6745) * maddest(coeff[-level])\n",
    "\n",
    "    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
    "\n",
    "    ret=pywt.waverec(coeff, wavelet, mode='per')\n",
    "    \n",
    "    return ret\n",
    "\n",
    "import librosa\n",
    "\n",
    "def spectrogram_from_eeg(parquet_path, display=False):\n",
    "    \n",
    "    # LOAD MIDDLE 50 SECONDS OF EEG SERIES\n",
    "    eeg = pd.read_parquet(parquet_path)\n",
    "    middle = (len(eeg)-10_000)//2\n",
    "    eeg = eeg.iloc[middle:middle+10_000]\n",
    "    \n",
    "    # VARIABLE TO HOLD SPECTROGRAM\n",
    "    img = np.zeros((100,300,4),dtype='float32')\n",
    "    \n",
    "    if display: plt.figure(figsize=(10,7))\n",
    "    signals = []\n",
    "    for k in range(4):\n",
    "        COLS = FEATS[k]\n",
    "        \n",
    "        for kk in range(4):\n",
    "            # FILL NANS\n",
    "            x1 = eeg[COLS[kk]].values\n",
    "            x2 = eeg[COLS[kk+1]].values\n",
    "            m = np.nanmean(x1)\n",
    "            if np.isnan(x1).mean()<1: x1 = np.nan_to_num(x1,nan=m)\n",
    "            else: x1[:] = 0\n",
    "            m = np.nanmean(x2)\n",
    "            if np.isnan(x2).mean()<1: x2 = np.nan_to_num(x2,nan=m)\n",
    "            else: x2[:] = 0\n",
    "                \n",
    "            # COMPUTE PAIR DIFFERENCES\n",
    "            x = x1 - x2\n",
    "\n",
    "            # DENOISE\n",
    "            if USE_WAVELET:\n",
    "                x = denoise(x, wavelet=USE_WAVELET)\n",
    "            signals.append(x)\n",
    "\n",
    "            # RAW SPECTROGRAM\n",
    "            mel_spec = librosa.feature.melspectrogram(y=x, sr=200, hop_length=len(x)//300, \n",
    "                  n_fft=1024, n_mels=100, fmin=0, fmax=20, win_length=128)\n",
    "            \n",
    "            # LOG TRANSFORM\n",
    "            width = (mel_spec.shape[1]//30)*30\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).astype(np.float32)[:,:width]\n",
    "            img[:,:,k] += mel_spec_db\n",
    "                \n",
    "        # AVERAGE THE 4 MONTAGE DIFFERENCES\n",
    "        img[:,:,k] /= 4.0\n",
    "        \n",
    "        if display:\n",
    "            plt.subplot(2,2,k+1)\n",
    "            plt.imshow(img[:,:,k],aspect='auto',origin='lower')\n",
    "            \n",
    "    if display: \n",
    "        plt.show()\n",
    "        plt.figure(figsize=(10,5))\n",
    "        offset = 0\n",
    "        for k in range(4):\n",
    "            if k>0: offset -= signals[3-k].min()\n",
    "            plt.plot(range(10_000),signals[k]+offset,label=NAMES[3-k])\n",
    "            offset += signals[3-k].max()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "653a88db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:48.724243Z",
     "iopub.status.busy": "2024-03-07T00:25:48.723854Z",
     "iopub.status.idle": "2024-03-07T00:25:48.742070Z",
     "shell.execute_reply": "2024-03-07T00:25:48.740995Z"
    },
    "papermill": {
     "duration": 0.031303,
     "end_time": "2024-03-07T00:25:48.744440",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.713137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape (1, 3)\n"
     ]
    }
   ],
   "source": [
    "if submission:\n",
    "    test = pd.read_csv(config.data_test_csv)\n",
    "    print('Test shape',test.shape)\n",
    "    test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88290d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:48.766226Z",
     "iopub.status.busy": "2024-03-07T00:25:48.765313Z",
     "iopub.status.idle": "2024-03-07T00:25:48.993946Z",
     "shell.execute_reply": "2024-03-07T00:25:48.993098Z"
    },
    "papermill": {
     "duration": 0.242194,
     "end_time": "2024-03-07T00:25:48.996382",
     "exception": false,
     "start_time": "2024-03-07T00:25:48.754188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 test spectrogram parquets\n",
      "0 , "
     ]
    }
   ],
   "source": [
    "# READ ALL SPECTROGRAMS\n",
    "if submission:\n",
    "    PATH2 = config.data_spectograms_test\n",
    "    files2 = os.listdir(PATH2)\n",
    "    print(f'There are {len(files2)} test spectrogram parquets')\n",
    "    \n",
    "    spectrograms2 = {}\n",
    "    for i,f in enumerate(files2):\n",
    "        if i%100==0: print(i,', ',end='')\n",
    "        tmp = pd.read_parquet(f'{PATH2}{f}')\n",
    "        name = int(f.split('.')[0])\n",
    "        spectrograms2[name] = tmp.iloc[:,1:].values\n",
    "    \n",
    "    # RENAME FOR DATA GENERATOR\n",
    "    test = test.rename({'spectrogram_id':'spec_id'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c3c2c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:25:49.028324Z",
     "iopub.status.busy": "2024-03-07T00:25:49.027565Z",
     "iopub.status.idle": "2024-03-07T00:26:00.143975Z",
     "shell.execute_reply": "2024-03-07T00:26:00.142423Z"
    },
    "papermill": {
     "duration": 11.131637,
     "end_time": "2024-03-07T00:26:00.148062",
     "exception": false,
     "start_time": "2024-03-07T00:25:49.016425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Test EEG to Spectrograms...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# READ ALL EEG SPECTROGRAMS\n",
    "if submission:\n",
    "    PATH2 = config.data_eeg_test\n",
    "    DISPLAY = 0\n",
    "    EEG_IDS2 = test.eeg_id.unique()\n",
    "    all_eegs2 = {}\n",
    "\n",
    "    print('Converting Test EEG to Spectrograms...'); print()\n",
    "    for i,eeg_id in enumerate(EEG_IDS2):\n",
    "        \n",
    "        # CREATE SPECTROGRAM FROM EEG PARQUET\n",
    "        img = spectrogram_from_eeg(f'{PATH2}{eeg_id}.parquet', i<DISPLAY)\n",
    "        all_eegs2[eeg_id] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a454011c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:26:00.193306Z",
     "iopub.status.busy": "2024-03-07T00:26:00.192100Z",
     "iopub.status.idle": "2024-03-07T00:26:00.226220Z",
     "shell.execute_reply": "2024-03-07T00:26:00.225076Z"
    },
    "papermill": {
     "duration": 0.062124,
     "end_time": "2024-03-07T00:26:00.230415",
     "exception": false,
     "start_time": "2024-03-07T00:26:00.168291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Test EEG parquets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# READ ALL RAW EEG SIGNALS\n",
    "if submission :\n",
    "    all_raw_eegs2 = {}\n",
    "    EEG_IDS2 = test.eeg_id.unique()\n",
    "    PATH2 = config.data_eeg_test\n",
    "\n",
    "    print('Processing Test EEG parquets...'); print()\n",
    "    for i,eeg_id in enumerate(EEG_IDS2):\n",
    "        \n",
    "        # SAVE EEG TO PYTHON DICTIONARY OF NUMPY ARRAYS\n",
    "        data = eeg_from_parquet(f'{PATH2}{eeg_id}.parquet')\n",
    "        all_raw_eegs2[eeg_id] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4e1061e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:26:00.277281Z",
     "iopub.status.busy": "2024-03-07T00:26:00.276882Z",
     "iopub.status.idle": "2024-03-07T00:26:00.287260Z",
     "shell.execute_reply": "2024-03-07T00:26:00.286093Z"
    },
    "papermill": {
     "duration": 0.035205,
     "end_time": "2024-03-07T00:26:00.289544",
     "exception": false,
     "start_time": "2024-03-07T00:26:00.254339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Submission ON TEST without ensemble\n",
    "if submission and not ENSEMBLE:\n",
    "    preds = []\n",
    "    \n",
    "    if DATA_TYPE=='raw':\n",
    "        test_gen = DataGenerator(test, mode='test', raw_eegs=all_raw_eegs2)\n",
    "        in_shape = (2000,8)\n",
    "    else:\n",
    "        test_gen = DataGenerator(test, mode='test', specs = spectrograms2, eeg_specs = all_eegs2)\n",
    "        in_shape = (512,512,3)\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_generator(generator=test_gen, \n",
    "                                               output_signature=(tf.TensorSpec(shape=in_shape, dtype=tf.float32),\n",
    "                                                                 tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    if DATA_TYPE=='raw':\n",
    "        model = build_wave_model()\n",
    "    else:\n",
    "        model = build_model()\n",
    "\n",
    "    for i in range(5):\n",
    "        print(f'Fold {i+1}')\n",
    "        model.load_weights(f'{config.futures_head_starters_models}model_{DATA_TYPE}_{VER}_{i}.weights.h5')\n",
    "        pred = model.predict(test_dataset, verbose=1)\n",
    "        preds.append(pred)\n",
    "        \n",
    "    pred = np.mean(preds,axis=0)\n",
    "    print('Test preds shape',pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ac77b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:26:00.309860Z",
     "iopub.status.busy": "2024-03-07T00:26:00.309511Z",
     "iopub.status.idle": "2024-03-07T00:27:11.765606Z",
     "shell.execute_reply": "2024-03-07T00:27:11.764506Z"
    },
    "papermill": {
     "duration": 71.468897,
     "end_time": "2024-03-07T00:27:11.768030",
     "exception": false,
     "start_time": "2024-03-07T00:26:00.299133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 603 variables. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer 'normalization_7' expected 3 variables, but received 0 variables during loading. Expected: ['normalization_7/mean:0', 'normalization_7/variance:0', 'normalization_7/count:0']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# print(f'{config.futures_head_starters_models}model_kaggle_{VERK}_{i}.weights.h5')\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures_head_starters_models\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mmodel_kaggle_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mVERK\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.weights.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m pred_kaggle \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_dataset_kaggle, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mfutures_head_starters_models\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mmodel_both_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVERB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/NoodleNappers/brain/venv/lib64/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/NoodleNappers/brain/venv/lib64/python3.10/site-packages/keras/src/engine/base_layer.py:3531\u001b[0m, in \u001b[0;36mLayer.load_own_variables\u001b[0;34m(self, store)\u001b[0m\n\u001b[1;32m   3529\u001b[0m all_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_weights \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_trainable_weights\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_vars):\n\u001b[0;32m-> 3531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables during loading. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3535\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[v\u001b[38;5;241m.\u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mall_vars]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3536\u001b[0m     )\n\u001b[1;32m   3537\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_vars):\n\u001b[1;32m   3538\u001b[0m     \u001b[38;5;66;03m# TODO(rchao): check shapes and raise errors.\u001b[39;00m\n\u001b[1;32m   3539\u001b[0m     v\u001b[38;5;241m.\u001b[39massign(store[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: Layer 'normalization_7' expected 3 variables, but received 0 variables during loading. Expected: ['normalization_7/mean:0', 'normalization_7/variance:0', 'normalization_7/count:0']"
     ]
    }
   ],
   "source": [
    "# Submission ON TEST with ensemble\n",
    "if submission and ENSEMBLE:\n",
    "    preds = []\n",
    "    test_gen_kaggle = DataGenerator(test, mode='test', data_type='kaggle', specs = spectrograms2, eeg_specs = all_eegs2)\n",
    "    test_dataset_kaggle = tf.data.Dataset.from_generator(generator=test_gen_kaggle, \n",
    "                                               output_signature=(tf.TensorSpec(shape=(512,512,3), dtype=tf.float32),\n",
    "                                                                 tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    test_gen_both = DataGenerator(test, mode='test', data_type='both', specs = spectrograms2, eeg_specs = all_eegs2)\n",
    "    test_dataset_both = tf.data.Dataset.from_generator(generator=test_gen_both, \n",
    "                                               output_signature=(tf.TensorSpec(shape=(512,512,3), dtype=tf.float32),\n",
    "                                                                 tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    test_gen_eeg = DataGenerator(test, mode='test', data_type='eeg', specs = spectrograms2, eeg_specs = all_eegs2)\n",
    "    test_dataset_eeg = tf.data.Dataset.from_generator(generator=test_gen_eeg, \n",
    "                                               output_signature=(tf.TensorSpec(shape=(512,512,3), dtype=tf.float32),\n",
    "                                                                 tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    test_gen_raw = DataGenerator(test, mode='test', data_type='raw', raw_eegs=all_raw_eegs2)\n",
    "    test_dataset_raw = tf.data.Dataset.from_generator(generator=test_gen_raw, \n",
    "                                               output_signature=(tf.TensorSpec(shape=(2000,8), dtype=tf.float32),\n",
    "                                                                 tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    " \n",
    "    # LB SCORE FOR EACH MODEL\n",
    "    lbs = 1 - np.array(LBs)\n",
    "    weights = lbs/lbs.sum()\n",
    "    model = build_model()\n",
    "    model_wave = build_wave_model()\n",
    "\n",
    "    for i in range(5):\n",
    "        print(f'Fold {i+1}')\n",
    "        # print(f'{config.futures_head_starters_models}model_kaggle_{VERK}_{i}.weights.h5')\n",
    "        model.load_weights(f'{config.futures_head_starters_models}model_kaggle_{VERK}_{i}.weights.h5')\n",
    "        pred_kaggle = model.predict(test_dataset_kaggle, verbose=1)\n",
    "        \n",
    "        model.load_weights(f'{config.futures_head_starters_models}model_both_{VERB}_{i}.weights.h5')\n",
    "        pred_both = model.predict(test_dataset_both, verbose=1)\n",
    "        \n",
    "        model.load_weights(f'{config.futures_head_starters_models}model_eeg_{VERE}_{i}.weights.h5')\n",
    "        pred_eeg = model.predict(test_dataset_eeg, verbose=1)\n",
    "        \n",
    "        model_wave.load_weights(f'{config.futures_head_starters_models}model_raw_{VERR}_{i}.weights.h5')\n",
    "        pred_raw = model_wave.predict(test_dataset_raw, verbose=1)\n",
    "        \n",
    "        pred = np.array([pred_both,pred_eeg,pred_kaggle,pred_raw])\n",
    "        pred = np.average(pred,axis=0,weights=weights)\n",
    "        preds.append(pred)\n",
    "        \n",
    "    pred = np.mean(preds,axis=0)\n",
    "    print('Test preds shape',pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23770e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:27:11.797585Z",
     "iopub.status.busy": "2024-03-07T00:27:11.797246Z",
     "iopub.status.idle": "2024-03-07T00:27:11.814274Z",
     "shell.execute_reply": "2024-03-07T00:27:11.813171Z"
    },
    "papermill": {
     "duration": 0.034103,
     "end_time": "2024-03-07T00:27:11.816432",
     "exception": false,
     "start_time": "2024-03-07T00:27:11.782329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m submission:\n\u001b[1;32m      2\u001b[0m     sub \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg_id\u001b[39m\u001b[38;5;124m'\u001b[39m:test\u001b[38;5;241m.\u001b[39meeg_id\u001b[38;5;241m.\u001b[39mvalues})\n\u001b[0;32m----> 3\u001b[0m     sub[TARGETS] \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\n\u001b[1;32m      4\u001b[0m     sub\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubmissionn shape\u001b[39m\u001b[38;5;124m'\u001b[39m,sub\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "if submission:\n",
    "    sub = pd.DataFrame({'eeg_id':test.eeg_id.values})\n",
    "    sub[TARGETS] = pred\n",
    "    sub.to_csv('submission.csv',index=False)\n",
    "    print('Submissionn shape',sub.shape)\n",
    "    print()\n",
    "    print(sub.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8b21a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T00:27:11.844145Z",
     "iopub.status.busy": "2024-03-07T00:27:11.843817Z",
     "iopub.status.idle": "2024-03-07T00:27:11.850891Z",
     "shell.execute_reply": "2024-03-07T00:27:11.850026Z"
    },
    "papermill": {
     "duration": 0.023382,
     "end_time": "2024-03-07T00:27:11.853603",
     "exception": false,
     "start_time": "2024-03-07T00:27:11.830221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n"
     ]
    }
   ],
   "source": [
    "# SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\n",
    "if submission:\n",
    "    print(sub.iloc[:,-6:].sum(axis=1).to_string())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "sourceId": 59093,
     "sourceType": "competition"
    },
    {
     "datasetId": 4297782,
     "sourceId": 7392775,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4317718,
     "sourceId": 7465251,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4407194,
     "sourceId": 7570342,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4417235,
     "sourceId": 7752365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4382744,
     "sourceId": 7752462,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 109.874197,
   "end_time": "2024-03-07T00:27:15.135058",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-07T00:25:25.260861",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
