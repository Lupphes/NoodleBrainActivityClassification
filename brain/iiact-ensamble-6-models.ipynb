{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7392733,"sourceType":"datasetVersion","datasetId":4297749},{"sourceId":7392775,"sourceType":"datasetVersion","datasetId":4297782},{"sourceId":7402356,"sourceType":"datasetVersion","datasetId":4304475},{"sourceId":7403069,"sourceType":"datasetVersion","datasetId":4304949},{"sourceId":7447509,"sourceType":"datasetVersion","datasetId":4334995},{"sourceId":7450712,"sourceType":"datasetVersion","datasetId":4336944},{"sourceId":7465251,"sourceType":"datasetVersion","datasetId":4317718},{"sourceId":7570342,"sourceType":"datasetVersion","datasetId":4407194},{"sourceId":7581697,"sourceType":"datasetVersion","datasetId":4413439},{"sourceId":7581715,"sourceType":"datasetVersion","datasetId":4413451},{"sourceId":7581720,"sourceType":"datasetVersion","datasetId":4413454},{"sourceId":7585255,"sourceType":"datasetVersion","datasetId":4415285},{"sourceId":7658681,"sourceType":"datasetVersion","datasetId":4465437},{"sourceId":7752462,"sourceType":"datasetVersion","datasetId":4382744},{"sourceId":7758086,"sourceType":"datasetVersion","datasetId":4536784},{"sourceId":7775601,"sourceType":"datasetVersion","datasetId":4527518},{"sourceId":7781969,"sourceType":"datasetVersion","datasetId":4494069},{"sourceId":7815824,"sourceType":"datasetVersion","datasetId":4578701},{"sourceId":7818473,"sourceType":"datasetVersion","datasetId":4474441},{"sourceId":7818976,"sourceType":"datasetVersion","datasetId":4417235},{"sourceId":7824772,"sourceType":"datasetVersion","datasetId":4585029},{"sourceId":7816407,"sourceType":"datasetVersion","datasetId":4430326,"isSourceIdPinned":false},{"sourceId":158958765,"sourceType":"kernelVersion"},{"sourceId":159333316,"sourceType":"kernelVersion"},{"sourceId":159396114,"sourceType":"kernelVersion"},{"sourceId":165733109,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IIACT: Ensemble for HMS Brain Comp by MLiP team\nThis is combination and ensemble notebook for Kaggle's HMS brain comp. based on [Previous Notebook](https://www.kaggle.com/code/luepoe/iiact-ensamble-features-head-starter) and [Thisone](https://www.kaggle.com/code/anilyagiz/hms-multiple-model-ensemble-4-notebooks-19b844) \n\n\n- https://www.kaggle.com/code/cdeotte/efficientnetb0-starter-lb-0-43 (That is the our model and our code)\n- https://www.kaggle.com/code/yunsuxiaozi/hms-baseline-resnet34d-512-512-inference-6-models\n- https://www.kaggle.com/code/andreasbis/hms-inference-lb-0-41\n- https://www.kaggle.com/code/nartaa/features-head-starter-lb-0-36\n\nExtra that needs to be added to this so its more vertasille:\n- [LB 0.46] DilatedInception WaveNet - Inference (https://www.kaggle.com/code/abaojiang/lb-0-46-dilatedinception-wavenet-inference/notebook?scriptVersionId=163448688)\n- CatBoost Starter - [LB 0.60]: [Notebook][https://www.kaggle.com/code/cdeotte/catboost-starter-lb-0-60] and our trained dataset on this\n\n**The Ensemble achieves LB 0.34** \n\nFeatures+Head Starter uses Chris Deotte's Kaggle dataset [here][1]. Also Uses Chris's EEG spectrograms [here][3] (modified version) \n\nThis notebook is based on the work of JIYUANZHANG, found [here](https://www.kaggle.com/code/kitsuha/3-model-ensemble-lb-0-37)","metadata":{"execution":{"iopub.execute_input":"2024-03-10T18:31:10.935579Z","iopub.status.busy":"2024-03-10T18:31:10.935226Z","iopub.status.idle":"2024-03-10T18:31:10.956617Z","shell.execute_reply":"2024-03-10T18:31:10.955380Z","shell.execute_reply.started":"2024-03-10T18:31:10.935549Z"}}},{"cell_type":"markdown","source":"# Intro and Config","metadata":{}},{"cell_type":"code","source":"%pip install d2l --no-index --find-links=file:///kaggle/input/d2l-package/d2l/\n%pip install /kaggle/input/brain-solver/brain_solver-0.9.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:49:56.645678Z","iopub.execute_input":"2024-03-12T13:49:56.646116Z","iopub.status.idle":"2024-03-12T13:50:44.369333Z","shell.execute_reply.started":"2024-03-12T13:49:56.646077Z","shell.execute_reply":"2024-03-12T13:50:44.368141Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: file:///kaggle/input/d2l-package/d2l/\nRequirement already satisfied: d2l in /opt/conda/lib/python3.10/site-packages (0.17.0)\nRequirement already satisfied: jupyter in /opt/conda/lib/python3.10/site-packages (from d2l) (1.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from d2l) (1.24.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from d2l) (3.7.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from d2l) (2.31.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from d2l) (2.0.3)\nRequirement already satisfied: notebook in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l) (6.5.5)\nRequirement already satisfied: qtconsole in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l) (5.5.1)\nRequirement already satisfied: jupyter-console in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l) (6.6.3)\nRequirement already satisfied: nbconvert in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l) (6.4.5)\nRequirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l) (6.25.1)\nRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l) (7.7.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->d2l) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->d2l) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->d2l) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->d2l) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->d2l) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->d2l) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->d2l) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->d2l) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->d2l) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->d2l) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->d2l) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->d2l) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->d2l) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->d2l) (2023.11.17)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->d2l) (1.16.0)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (0.1.4)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (1.6.7.post1)\nRequirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (8.14.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (5.3.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (1.5.6)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (5.9.3)\nRequirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (24.0.1)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (6.3.3)\nRequirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l) (5.9.0)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter->d2l) (0.2.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter->d2l) (3.6.6)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter->d2l) (3.0.8)\nRequirement already satisfied: prompt-toolkit>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from jupyter-console->jupyter->d2l) (3.0.39)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.10/site-packages (from jupyter-console->jupyter->d2l) (2.16.1)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (0.8.4)\nRequirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (3.1.2)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (0.2.2)\nRequirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (5.9.2)\nRequirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (0.4)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (6.0.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (4.12.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (0.5.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l) (2.1.3)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->d2l) (21.3.0)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->d2l) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->d2l) (0.17.1)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->d2l) (0.17.1)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->d2l) (1.0.0)\nRequirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from qtconsole->jupyter->d2l) (2.4.1)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l) (0.19.0)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l) (0.7.5)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l) (0.6.2)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l) (4.8.0)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->d2l) (4.1.0)\nRequirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook->jupyter->d2l) (2.12.3)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook->jupyter->d2l) (0.2.3)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.4->nbconvert->jupyter->d2l) (2.18.0)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.4->nbconvert->jupyter->d2l) (4.19.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->d2l) (0.2.6)\nRequirement already satisfied: ptyprocess in /opt/conda/lib/python3.10/site-packages (from terminado>=0.8.3->notebook->jupyter->d2l) (0.7.0)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook->jupyter->d2l) (21.2.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter->d2l) (2.3.2.post1)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->d2l) (0.8.3)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l) (23.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l) (0.9.2)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (3.7.1)\nRequirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (0.4.4)\nRequirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (6.5.0)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (1.6.2)\nRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l) (1.15.1)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l) (0.2.2)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (1.1.3)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l) (2.21)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (2.0.7)\nRequirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (6.0.1)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l) (2.0)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l) (1.2.3)\nNote: you may need to restart the kernel to use updated packages.\nProcessing /kaggle/input/brain-solver/brain_solver-0.9.0-py3-none-any.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (1.24.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (2.0.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (2.0.0)\nRequirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (1.3.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (3.7.4)\nRequirement already satisfied: PyWavelets in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (1.4.1)\nRequirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (0.10.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (1.11.4)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (2.1.3)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (0.15.1)\nRequirement already satisfied: d2l in /opt/conda/lib/python3.10/site-packages (from brain-solver==0.9.0) (0.17.0)\nRequirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations->brain-solver==0.9.0) (0.21.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations->brain-solver==0.9.0) (6.0.1)\nRequirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations->brain-solver==0.9.0) (0.0.4)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations->brain-solver==0.9.0) (4.9.0.80)\nRequirement already satisfied: jupyter in /opt/conda/lib/python3.10/site-packages (from d2l->brain-solver==0.9.0) (1.0.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from d2l->brain-solver==0.9.0) (2.31.0)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (1.3.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (0.57.1)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (0.12.1)\nRequirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (1.8.0)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (0.3.7)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (4.5.0)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (0.3)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->brain-solver==0.9.0) (1.0.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->brain-solver==0.9.0) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->brain-solver==0.9.0) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->brain-solver==0.9.0) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->brain-solver==0.9.0) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->brain-solver==0.9.0) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->brain-solver==0.9.0) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->brain-solver==0.9.0) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->brain-solver==0.9.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->brain-solver==0.9.0) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->brain-solver==0.9.0) (2023.3)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning->brain-solver==0.9.0) (4.66.1)\nRequirement already satisfied: fsspec[http]>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning->brain-solver==0.9.0) (2023.12.2)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning->brain-solver==0.9.0) (1.2.1)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning->brain-solver==0.9.0) (0.10.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->brain-solver==0.9.0) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->brain-solver==0.9.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->brain-solver==0.9.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->brain-solver==0.9.0) (3.1.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->brain-solver==0.9.0) (3.8.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning->brain-solver==0.9.0) (68.1.2)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa->brain-solver==0.9.0) (0.40.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa->brain-solver==0.9.0) (4.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->brain-solver==0.9.0) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->d2l->brain-solver==0.9.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->d2l->brain-solver==0.9.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->d2l->brain-solver==0.9.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->d2l->brain-solver==0.9.0) (2023.11.17)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->brain-solver==0.9.0) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->brain-solver==0.9.0) (2023.8.12)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa->brain-solver==0.9.0) (3.2.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->brain-solver==0.9.0) (1.15.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->brain-solver==0.9.0) (2.1.3)\nRequirement already satisfied: notebook in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l->brain-solver==0.9.0) (6.5.5)\nRequirement already satisfied: qtconsole in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l->brain-solver==0.9.0) (5.5.1)\nRequirement already satisfied: jupyter-console in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l->brain-solver==0.9.0) (6.6.3)\nRequirement already satisfied: nbconvert in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l->brain-solver==0.9.0) (6.4.5)\nRequirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l->brain-solver==0.9.0) (6.25.1)\nRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (from jupyter->d2l->brain-solver==0.9.0) (7.7.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->brain-solver==0.9.0) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->brain-solver==0.9.0) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->brain-solver==0.9.0) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->brain-solver==0.9.0) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->brain-solver==0.9.0) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->brain-solver==0.9.0) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->brain-solver==0.9.0) (1.3.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->brain-solver==0.9.0) (2.21)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (0.1.4)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (1.6.7.post1)\nRequirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (8.14.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (5.3.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (1.5.6)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (5.9.3)\nRequirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (24.0.1)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (6.3.3)\nRequirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->d2l->brain-solver==0.9.0) (5.9.0)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter->d2l->brain-solver==0.9.0) (0.2.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter->d2l->brain-solver==0.9.0) (3.6.6)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter->d2l->brain-solver==0.9.0) (3.0.8)\nRequirement already satisfied: prompt-toolkit>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from jupyter-console->jupyter->d2l->brain-solver==0.9.0) (3.0.39)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.10/site-packages (from jupyter-console->jupyter->d2l->brain-solver==0.9.0) (2.16.1)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l->brain-solver==0.9.0) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l->brain-solver==0.9.0) (0.2.2)\nRequirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l->brain-solver==0.9.0) (5.9.2)\nRequirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l->brain-solver==0.9.0) (0.4)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l->brain-solver==0.9.0) (6.0.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l->brain-solver==0.9.0) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l->brain-solver==0.9.0) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l->brain-solver==0.9.0) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l->brain-solver==0.9.0) (4.12.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->d2l->brain-solver==0.9.0) (0.5.13)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->d2l->brain-solver==0.9.0) (21.3.0)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->d2l->brain-solver==0.9.0) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->d2l->brain-solver==0.9.0) (0.17.1)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->d2l->brain-solver==0.9.0) (0.17.1)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->d2l->brain-solver==0.9.0) (1.0.0)\nRequirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from qtconsole->jupyter->d2l->brain-solver==0.9.0) (2.4.1)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l->brain-solver==0.9.0) (0.2.0)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l->brain-solver==0.9.0) (0.19.0)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l->brain-solver==0.9.0) (0.7.5)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l->brain-solver==0.9.0) (0.6.2)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l->brain-solver==0.9.0) (4.8.0)\nRequirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (2.12.3)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (0.2.3)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (2.18.0)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (4.19.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->d2l->brain-solver==0.9.0) (0.2.6)\nRequirement already satisfied: ptyprocess in /opt/conda/lib/python3.10/site-packages (from terminado>=0.8.3->notebook->jupyter->d2l->brain-solver==0.9.0) (0.7.0)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook->jupyter->d2l->brain-solver==0.9.0) (21.2.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (2.3.2.post1)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert->jupyter->d2l->brain-solver==0.9.0) (0.5.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->d2l->brain-solver==0.9.0) (0.8.3)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (0.9.2)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (3.7.1)\nRequirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (0.4.4)\nRequirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (6.5.0)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (1.6.2)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l->brain-solver==0.9.0) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l->brain-solver==0.9.0) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l->brain-solver==0.9.0) (0.2.2)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (1.1.3)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (2.0.7)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l->brain-solver==0.9.0) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (2.0)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter->d2l->brain-solver==0.9.0) (1.2.3)\nbrain-solver is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Config Class Summary\n\nThe `Config` class manages configurations for a brain activity classification project. It includes:\n\n- **Data and Model Paths**: Centralizes paths for data (e.g., EEG, spectrograms) and model checkpoints.\n- **Training Parameters**: Configures training details like epochs, batch size, and learning rate.\n- **Feature Flags**: Toggles for using wavelets, spectrograms, and reading options.\n\nDesigned for easy adjustments to facilitate model development and experimentation.","metadata":{}},{"cell_type":"code","source":"from brain_solver import Config\n\n# full_path = \"/home/osloup/NoodleNappers/brain/data/\" # Luppo\n# full_path = \"C:/Users/tygof/Documents/Semester 8/MLiP/NoodleNappers/brain/data/\" # Tygo\n# full_path = \"C:/Users/dahbl/Documents/TrueDocs/Uni/Year 4/Semester 2/Machine Learning in Practice/brain/brain/data/\" # Dick\n# config = Config(full_path,  full_path + \"out/\", USE_EEG_SPECTROGRAMS=True, USE_KAGGLE_SPECTROGRAMS=True, should_read_brain_spectograms=False, should_read_eeg_spectrogram_files=False, USE_PRETRAINED_MODEL=True, FINE_TUNE=True)\n\n# Kaggle Pull\nfull_path = \"/kaggle/input/\"\nconfig = Config(\n    full_path,\n    \"/kaggle/working/\",\n    USE_EEG_SPECTROGRAMS=False,\n    VER=5,\n    USE_KAGGLE_SPECTROGRAMS=True,\n    should_read_brain_spectograms=False,\n    should_read_eeg_spectrogram_files=False,\n    USE_PRETRAINED_MODEL=False,\n)\n\nimport sys\n\nsys.path.append(full_path + \"kaggle-kl-div\")\nfrom kaggle_kl_div import score","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:44.374272Z","iopub.execute_input":"2024-03-12T13:50:44.374600Z","iopub.status.idle":"2024-03-12T13:50:49.173438Z","shell.execute_reply.started":"2024-03-12T13:50:44.374565Z","shell.execute_reply":"2024-03-12T13:50:49.172509Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pytorch_lightning as pl\n\n# Create Output folder if does not exist\nif not os.path.exists(config.output_path):\n    os.makedirs(config.output_path)\n\n# Initialize random environment\npl.seed_everything(config.seed, workers=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:49.174828Z","iopub.execute_input":"2024-03-12T13:50:49.175465Z","iopub.status.idle":"2024-03-12T13:50:49.186344Z","shell.execute_reply.started":"2024-03-12T13:50:49.175424Z","shell.execute_reply":"2024-03-12T13:50:49.185365Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"2024"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model 1 - Starter/Ours","metadata":{}},{"cell_type":"code","source":"import os, sys\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom brain_solver import (\n    Helpers as hp,\n    Trainer as tr,\n    BrainModel as br,\n    EEGDataset,\n    Network,\n)\nfrom brain_solver import Wav2Vec2 as w2v\nfrom brain_solver import Filters, FilterType\nfrom transformers.utils import logging\nfrom tqdm import tqdm\n\n# Suppress warnings if desired\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity(logging.CRITICAL)\n\n# Setup for CUDA device selection\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:49.189396Z","iopub.execute_input":"2024-03-12T13:50:49.189777Z","iopub.status.idle":"2024-03-12T13:50:49.246037Z","shell.execute_reply.started":"2024-03-12T13:50:49.189726Z","shell.execute_reply":"2024-03-12T13:50:49.245041Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"train_df: pd.DataFrame = hp.load_csv(config.data_train_csv)\ntest_df = pd.read_csv(config.data_test_csv)\n\nif train_df is None:\n    print(\"Failed to load the CSV file.\")\n    exit()\nelse:\n    EEG_IDS = train_df.eeg_id.unique()\n    TARGETS = train_df.columns[-6:]\n    TARS = {\"Seizure\": 0, \"LPD\": 1, \"GPD\": 2, \"LRDA\": 3, \"GRDA\": 4, \"Other\": 5}\n    TARS_INV = {x: y for y, x in TARS.items()}\n    print(\"Train shape:\", train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:49.247296Z","iopub.execute_input":"2024-03-12T13:50:49.247736Z","iopub.status.idle":"2024-03-12T13:50:49.437738Z","shell.execute_reply.started":"2024-03-12T13:50:49.247674Z","shell.execute_reply":"2024-03-12T13:50:49.436761Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Train shape: (106800, 15)\n","output_type":"stream"}]},{"cell_type":"code","source":"spectrograms = hp.read_spectrograms(\n    path=config.data_spectograms_test,\n    data_path_train_on_brain_spectograms_dataset_specs=None,\n    read_files=True,\n)\n\n# Continue with renaming for DataLoader\ntest_df = test_df.rename({\"spectrogram_id\": \"spec_id\"}, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:49.439341Z","iopub.execute_input":"2024-03-12T13:50:49.439774Z","iopub.status.idle":"2024-03-12T13:50:49.544799Z","shell.execute_reply.started":"2024-03-12T13:50:49.439735Z","shell.execute_reply":"2024-03-12T13:50:49.543657Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"There are 1 spectrogram parquets\n0 , ","output_type":"stream"}]},{"cell_type":"code","source":"# READ ALL EEG SPECTROGRAMS\nDISPLAY = 1\nEEG_IDS2 = test_df.eeg_id.unique()\nall_eegs2 = {}\n\nprint(\"Converting Test EEG to Spectrograms...\")\nfor i, eeg_id in enumerate(EEG_IDS2):\n    all_eegs2[eeg_id] = hp.spectrogram_from_eeg(\n        f\"{config.data_eeg_test}{eeg_id}.parquet\", False, config.use_wavelet\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:49.546120Z","iopub.execute_input":"2024-03-12T13:50:49.546486Z","iopub.status.idle":"2024-03-12T13:50:51.381005Z","shell.execute_reply.started":"2024-03-12T13:50:49.546455Z","shell.execute_reply":"2024-03-12T13:50:51.379678Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Converting Test EEG to Spectrograms...\n","output_type":"stream"}]},{"cell_type":"code","source":"# INFER EFFICIENTNET ON TEST\npreds = []\ntest_ds = EEGDataset(\n    test_df, specs=spectrograms, eeg_specs=all_eegs2, targets=TARGETS, mode=\"test\"\n)\ntest_loader = DataLoader(test_ds, shuffle=False, batch_size=64, num_workers=3)\n\nfor i in range(5):\n    print(\"#\" * 25)\n    print(f\"### Testing Fold {i+1}\")\n\n    ckpt_file = (\n        f\"EffNet_version{config.VER}_fold{i+1}.pth\"\n        if config.trained_model_path is None\n        else f\"{config.trained_model_path}/EffNet_v{config.VER}_f{i}.ckpt\"\n    )\n    model = torch.load(config.full_path + \"trained-model-effnet-mlip9/\" + ckpt_file)\n    model = model.to(device).eval()\n    fold_preds = []\n\n    with torch.inference_mode():\n        for test_batch in test_loader:\n            test_batch = test_batch.to(device)\n            pred = torch.softmax(model(test_batch), dim=1).cpu().numpy()\n            fold_preds.append(pred)\n\n            # Delete variables not needed to free up memory\n            del test_batch, pred\n            gc.collect()  # Manually collect garbage\n\n            if device.type == \"cuda\":  # Optionally clear CUDA cache if using GPU\n                torch.cuda.empty_cache()\n\n        fold_preds = np.concatenate(fold_preds)\n\n    preds.append(fold_preds)\n\n    del model\n    gc.collect()\n    if device.type == \"cuda\":\n        torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:51.382446Z","iopub.execute_input":"2024-03-12T13:50:51.383307Z","iopub.status.idle":"2024-03-12T13:50:55.655797Z","shell.execute_reply.started":"2024-03-12T13:50:51.383270Z","shell.execute_reply":"2024-03-12T13:50:55.654758Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"#########################\n### Testing Fold 1\n#########################\n### Testing Fold 2\n#########################\n### Testing Fold 3\n#########################\n### Testing Fold 4\n#########################\n### Testing Fold 5\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_model1 = np.mean(preds, axis=0)\nprint()\nprint(\"Test preds shape\", pred_model1.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:55.657427Z","iopub.execute_input":"2024-03-12T13:50:55.657819Z","iopub.status.idle":"2024-03-12T13:50:55.664262Z","shell.execute_reply.started":"2024-03-12T13:50:55.657774Z","shell.execute_reply":"2024-03-12T13:50:55.663381Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\nTest preds shape (1, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Continue with renaming for DataLoader\ntest_df = test_df.rename({\"spec_id\": \"spectrogram_id\"}, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:55.665423Z","iopub.execute_input":"2024-03-12T13:50:55.665720Z","iopub.status.idle":"2024-03-12T13:50:55.677924Z","shell.execute_reply.started":"2024-03-12T13:50:55.665671Z","shell.execute_reply":"2024-03-12T13:50:55.677188Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# sub1 = pd.DataFrame({\"eeg_id\": test_df.eeg_id.values})\n# sub1[TARGETS] = pred_model1[0]\n# sub1.to_csv(\"submission_model1.csv\", index=False)\n# print(\"Submissionn shape\", sub1.shape)\n# sub1.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:55.679213Z","iopub.execute_input":"2024-03-12T13:50:55.679488Z","iopub.status.idle":"2024-03-12T13:50:55.691638Z","shell.execute_reply.started":"2024-03-12T13:50:55.679464Z","shell.execute_reply":"2024-03-12T13:50:55.690848Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"pred_model1","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:55.692719Z","iopub.execute_input":"2024-03-12T13:50:55.693087Z","iopub.status.idle":"2024-03-12T13:50:55.706315Z","shell.execute_reply.started":"2024-03-12T13:50:55.693061Z","shell.execute_reply":"2024-03-12T13:50:55.705359Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([[0.02940221, 0.04890793, 0.00278009, 0.3094476 , 0.08677585,\n        0.52268636]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# # SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\n# sub1.iloc[:, -6:].sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:55.712258Z","iopub.execute_input":"2024-03-12T13:50:55.712622Z","iopub.status.idle":"2024-03-12T13:50:55.718659Z","shell.execute_reply.started":"2024-03-12T13:50:55.712594Z","shell.execute_reply":"2024-03-12T13:50:55.717892Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"del spectrograms, all_eegs2\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:55.719823Z","iopub.execute_input":"2024-03-12T13:50:55.720139Z","iopub.status.idle":"2024-03-12T13:50:55.924540Z","shell.execute_reply.started":"2024-03-12T13:50:55.720111Z","shell.execute_reply":"2024-03-12T13:50:55.923595Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model 2 - ResNet34d","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\n\nimport random\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:55.925780Z","iopub.execute_input":"2024-03-12T13:50:55.926056Z","iopub.status.idle":"2024-03-12T13:50:55.933310Z","shell.execute_reply.started":"2024-03-12T13:50:55.926032Z","shell.execute_reply":"2024-03-12T13:50:55.932418Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"image_transform = transforms.Resize((512, 512))","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:55.934595Z","iopub.execute_input":"2024-03-12T13:50:55.935005Z","iopub.status.idle":"2024-03-12T13:50:55.947343Z","shell.execute_reply.started":"2024-03-12T13:50:55.934971Z","shell.execute_reply":"2024-03-12T13:50:55.946488Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"models = []\nfor i in range(config.num_folds):\n    model = torch.load(f\"{config.resnet34d}HMS_resnet_fold{i}.pth\")\n    models.append(model)\nmodel = torch.load(\n    f\"{config.full_path}hms-baseline-resnet34d-512-512-training/HMS_resnet.pth\"\n)\nmodels.append(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:55.948651Z","iopub.execute_input":"2024-03-12T13:50:55.949102Z","iopub.status.idle":"2024-03-12T13:50:56.223868Z","shell.execute_reply.started":"2024-03-12T13:50:55.949068Z","shell.execute_reply":"2024-03-12T13:50:56.219964Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_folds\u001b[49m):\n\u001b[1;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mresnet34d\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mHMS_resnet_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(model)\n","\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'num_folds'"],"ename":"AttributeError","evalue":"'Config' object has no attribute 'num_folds'","output_type":"error"}]},{"cell_type":"code","source":"def seed_everything(seed):\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\nseed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.224570Z","iopub.status.idle":"2024-03-12T13:50:56.224938Z","shell.execute_reply.started":"2024-03-12T13:50:56.224760Z","shell.execute_reply":"2024-03-12T13:50:56.224778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(f\"{config.competition_data_path}/sample_submission.csv\")\nsubmission = submission.merge(test_df, on=\"eeg_id\", how=\"left\")\nsubmission[\"path\"] = submission[\"spectrogram_id\"].apply(\n    lambda x: config.data_spectograms_test + str(x) + \".parquet\"\n)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.226254Z","iopub.status.idle":"2024-03-12T13:50:56.226566Z","shell.execute_reply.started":"2024-03-12T13:50:56.226410Z","shell.execute_reply":"2024-03-12T13:50:56.226424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = submission[\"path\"].values\npred_model2 = []\nfor path in paths:\n    eps = 1e-6\n    data = pd.read_parquet(path)\n\n    data = data.fillna(-1).values[:, 1:].T\n    data = data[:, 0:300]  # (400,300)\n    data = np.clip(data, np.exp(-6), np.exp(10))\n    data = np.log(data)\n    data_mean = data.mean(axis=(0, 1))\n    data_std = data.std(axis=(0, 1))\n    data = (data - data_mean) / (data_std + eps)\n    data_tensor = torch.unsqueeze(torch.Tensor(data), dim=0)\n    data = image_transform(data_tensor)\n    test_pred = []\n    for model in models:\n        model.eval()\n        with torch.no_grad():\n            pred = F.softmax(model(data.unsqueeze(0)))[0]\n            pred = pred.detach().cpu().numpy()\n        test_pred.append(pred)\n    test_pred = np.array(test_pred).mean(axis=0)\n    pred_model2.append(test_pred)\npred_model2 = np.array(pred_model2)\npred_model2","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.228120Z","iopub.status.idle":"2024-03-12T13:50:56.228459Z","shell.execute_reply.started":"2024-03-12T13:50:56.228289Z","shell.execute_reply":"2024-03-12T13:50:56.228305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub2 = pd.read_csv(f\"{config.competition_data_path}/sample_submission.csv\")\nlabels = [\"seizure\", \"lpd\", \"gpd\", \"lrda\", \"grda\", \"other\"]\nfor i in range(len(labels)):\n    sub2[f\"{labels[i]}_vote\"] = pred_model2[:, i]\nsub2.head()\n# sub2.to_csv(\"submission_model2.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.230069Z","iopub.status.idle":"2024-03-12T13:50:56.230387Z","shell.execute_reply.started":"2024-03-12T13:50:56.230225Z","shell.execute_reply":"2024-03-12T13:50:56.230240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 3 - ResNet34d, EfficientNetB0 and EfficientnetB1","metadata":{}},{"cell_type":"code","source":"# Importing essential libraries\nimport gc\nimport os\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\n# PyTorch for deep learning\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n# torchvision for image processing and augmentation\nimport torchvision.transforms as transforms\n\n# Suppressing minor warnings to keep the output clean\nwarnings.filterwarnings(\"ignore\", category=Warning)\n\n# Reclaim memory no longer in use.\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.231497Z","iopub.status.idle":"2024-03-12T13:50:56.231858Z","shell.execute_reply.started":"2024-03-12T13:50:56.231663Z","shell.execute_reply":"2024-03-12T13:50:56.231679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config.seed = 42\nimage_transform = transforms.Resize((512, 512))\n\n\n# Set the seed for reproducibility across multiple libraries\ndef set_seed(seed):\n    print(f\"Setting seed non standard one: {seed}\")\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\nset_seed(config.seed)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.232746Z","iopub.status.idle":"2024-03-12T13:50:56.233053Z","shell.execute_reply.started":"2024-03-12T13:50:56.232899Z","shell.execute_reply":"2024-03-12T13:50:56.232914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and store the trained models for each fold into a list\nmodels = []\n\n# Load ResNet34d\nfor i in range(config.num_folds):\n    # Create the same model architecture as during training\n    model_resnet = timm.create_model(\n        \"resnet34d\", pretrained=False, num_classes=6, in_chans=1\n    )\n\n    # Load the trained weights from the corresponding file\n    model_resnet.load_state_dict(\n        torch.load(\n            f\"{config.train_resnet34d}resnet34d_fold{i}.pth\",\n            map_location=torch.device(\"cpu\"),\n        )\n    )\n\n    # Append the loaded model to the models list\n    models.append(model_resnet)\n\n# Reclaim memory no longer in use.\ngc.collect()\n\n# Load EfficientNetB0\nfor j in range(config.num_folds):\n    # Create the same model architecture as during training\n    model_effnet_b0 = timm.create_model(\n        \"efficientnet_b0\", pretrained=False, num_classes=6, in_chans=1\n    )\n\n    # Load the trained weights from the corresponding file\n    model_effnet_b0.load_state_dict(\n        torch.load(\n            f\"{config.efficientnetb0}efficientnet_b0_fold{j}.pth\",\n            map_location=torch.device(\"cpu\"),\n        )\n    )\n\n    # Append the loaded model to the models list\n    models.append(model_effnet_b0)\n\n# Reclaim memory no longer in use.\ngc.collect()\n\n# Load EfficientNetB1\nfor k in range(config.num_folds):\n    # Create the same model architecture as during training\n    model_effnet_b1 = timm.create_model(\n        \"efficientnet_b1\", pretrained=False, num_classes=6, in_chans=1\n    )\n\n    # Load the trained weights from the corresponding file\n    model_effnet_b1.load_state_dict(\n        torch.load(\n            f\"{config.efficientnetb1}efficientnet_b1_fold{k}.pth\",\n            map_location=torch.device(\"cpu\"),\n        )\n    )\n\n    # Append the loaded model to the models list\n    models.append(model_effnet_b1)\n\n# Reclaim memory no longer in use.\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.234431Z","iopub.status.idle":"2024-03-12T13:50:56.234889Z","shell.execute_reply.started":"2024-03-12T13:50:56.234641Z","shell.execute_reply":"2024-03-12T13:50:56.234663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load test data and sample submission dataframe\ntest_df = pd.read_csv(config.data_test_csv)\nsubmission = pd.read_csv(f\"{config.competition_data_path}/sample_submission.csv\")\n\n# Merge the submission dataframe with the test data on EEG IDs\nsubmission = submission.merge(test_df, on=\"eeg_id\", how=\"left\")\n\n# Generate file paths for each spectrogram based on the EEG data in the submission dataframe\nsubmission[\"path\"] = submission[\"spectrogram_id\"].apply(\n    lambda x: f\"{config.data_spectograms_test}{x}.parquet\"\n)\n\n# Display the first few rows of the submission dataframe\ndisplay(submission.head())\n\n# Reclaim memory no longer in use\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.236172Z","iopub.status.idle":"2024-03-12T13:50:56.236655Z","shell.execute_reply.started":"2024-03-12T13:50:56.236414Z","shell.execute_reply":"2024-03-12T13:50:56.236437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the weights for each model\nweight_resnet34d = 0.26\nweight_effnetb0 = 0.48\nweight_effnetb1 = 0.26\n\n# Get file paths for test spectrograms\npaths = submission[\"path\"].values\npred_model3 = []\n\n# Generate predictions for each spectrogram using all models\nfor path in paths:\n    eps = 1e-6\n    # Read and preprocess spectrogram data\n    data = pd.read_parquet(path)\n    data = data.fillna(-1).values[:, 1:].T\n    data = np.clip(data, np.exp(-6), np.exp(10))\n    data = np.log(data)\n\n    # Normalize the data\n    data_mean = data.mean(axis=(0, 1))\n    data_std = data.std(axis=(0, 1))\n    data = (data - data_mean) / (data_std + eps)\n    data_tensor = torch.unsqueeze(torch.Tensor(data), dim=0)\n    data = image_transform(data_tensor)\n\n    test_pred = []\n\n    # Generate predictions using all models\n    for model in models:\n        model.eval()\n        with torch.no_grad():\n            pred = F.softmax(model(data.unsqueeze(0)))[0]\n            pred = pred.detach().cpu().numpy()\n        test_pred.append(pred)\n\n    # Combine predictions from all models using weighted voting\n    weighted_pred = (\n        weight_resnet34d * np.mean(test_pred[: config.num_folds], axis=0)\n        + weight_effnetb0\n        * np.mean(test_pred[config.num_folds : 2 * config.num_folds], axis=0)\n        + weight_effnetb1 * np.mean(test_pred[2 * config.num_folds :], axis=0)\n    )\n\n    pred_model3.append(weighted_pred)\n\n# Convert the list of predictions to a NumPy array for further processing\npred_model3 = np.array(pred_model3)\n\n\n# Reclaim memory no longer in use\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.238608Z","iopub.status.idle":"2024-03-12T13:50:56.239069Z","shell.execute_reply.started":"2024-03-12T13:50:56.238841Z","shell.execute_reply":"2024-03-12T13:50:56.238863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 3\n# eeg_id_values = [3911565283]\n# TARGETS = [\n#     \"seizure_vote\",\n#     \"lpd_vote\",\n#     \"gpd_vote\",\n#     \"lrda_vote\",\n#     \"grda_vote\",\n#     \"other_vote\",\n# ]\n# sub3 = pd.DataFrame({\"eeg_id\": eeg_id_values})\n# sub3[TARGETS] = pred_model3\n# print(\"Submission shape\", sub3.shape)\n# sub3.to_csv(\"submission_model3.csv\", index=False)\n# sub3.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.240513Z","iopub.status.idle":"2024-03-12T13:50:56.240985Z","shell.execute_reply.started":"2024-03-12T13:50:56.240748Z","shell.execute_reply":"2024-03-12T13:50:56.240771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_model3","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.242262Z","iopub.status.idle":"2024-03-12T13:50:56.242728Z","shell.execute_reply.started":"2024-03-12T13:50:56.242484Z","shell.execute_reply":"2024-03-12T13:50:56.242505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 4 - Features+Head Ensemble ","metadata":{}},{"cell_type":"code","source":"import os, random\nimport tensorflow as tf\nimport tensorflow\nimport tensorflow.keras.backend as K\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import load_model\n\nLOAD_BACKBONE_FROM = config.efficientnetb_tf_keras\nLOAD_MODELS_FROM = config.futures_head_starters_models\nHMS_PATH = config.competition_data_path\nVER = 50\nDATA_TYPE = \"KER\"  # K|E|R|KE|KR|ER|KER\nUSE_PROCESSED = True  # Use processed downsampled Raw EEG\nsubmission = True\n\n# Setup for ensemble\nENSEMBLE = True\nLBs = [\n    0.41,\n    0.39,\n    0.41,\n    0.37,\n    0.39,\n    0.38,\n    0.36,\n]  # K|E|R|KE|KR|ER|KER for weighted ensemble we use LBs of each model\nVER_K = 43  # Kaggle's spectrogram model version\nVER_E = 42  # EEG's spectrogram model version\nVER_R = 37  # EEG's Raw wavenet model version, trained on single GPU\nVER_KE = 47  # Kaggle's and EEG's spectrogram model version\nVER_KR = 48  # Kaggle's spectrogram and Raw model version\nVER_ER = 49  # EEG's spectrogram and Raw model version\nVER_KER = 50  # EEG's, Kaggle's spectrograms and Raw model version\n\nnp.random.seed(42)\nrandom.seed(42)\ntf.random.set_seed(42)\n\n# USE SINGLE GPU, MULTIPLE GPUS\ngpus = tf.config.list_physical_devices(\"GPU\")\n# WE USE MIXED PRECISION\ntf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nif len(gpus) > 1:\n    strategy = tf.distribute.MirroredStrategy()\n    print(f\"Using {len(gpus)} GPUs\")\nelse:\n    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n    print(f\"Using {len(gpus)} GPU\")\n\nTARGETS = [\n    \"seizure_vote\",\n    \"lpd_vote\",\n    \"gpd_vote\",\n    \"lrda_vote\",\n    \"grda_vote\",\n    \"other_vote\",\n]","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.244003Z","iopub.status.idle":"2024-03-12T13:50:56.244335Z","shell.execute_reply.started":"2024-03-12T13:50:56.244172Z","shell.execute_reply":"2024-03-12T13:50:56.244188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DATA GENERATOR\nThis data generator outputs 512x512x3, the spectrogram and eeg images are concatenated all togother in a single image. For using data augmention you can set `augment = True` when creating the train data generator.","metadata":{}},{"cell_type":"code","source":"import albumentations as albu\nfrom scipy.signal import butter, lfilter\nimport librosa\n\nFEATS2 = [\"Fp1\", \"T3\", \"C3\", \"O1\", \"Fp2\", \"C4\", \"T4\", \"O2\"]\nFEAT2IDX = {x: y for x, y in zip(FEATS2, range(len(FEATS2)))}\nFEATS = [\n    [\"Fp1\", \"F7\", \"T3\", \"T5\", \"O1\"],\n    [\"Fp1\", \"F3\", \"C3\", \"P3\", \"O1\"],\n    [\"Fp2\", \"F8\", \"T4\", \"T6\", \"O2\"],\n    [\"Fp2\", \"F4\", \"C4\", \"P4\", \"O2\"],\n]\n\n\nclass DataGenerator:\n    \"Generates data for Keras\"\n\n    def __init__(\n        self,\n        data,\n        specs=None,\n        eeg_specs=None,\n        raw_eegs=None,\n        augment=False,\n        mode=\"train\",\n        data_type=DATA_TYPE,\n    ):\n        self.data = data\n        self.augment = augment\n        self.mode = mode\n        self.data_type = data_type\n        self.specs = specs\n        self.eeg_specs = eeg_specs\n        self.raw_eegs = raw_eegs\n        self.on_epoch_end()\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, index):\n        X, y = self.data_generation(index)\n        if self.augment:\n            X = self.augmentation(X)\n        return X, y\n\n    def __call__(self):\n        for i in range(self.__len__()):\n            yield self.__getitem__(i)\n\n            if i == self.__len__() - 1:\n                self.on_epoch_end()\n\n    def on_epoch_end(self):\n        if self.mode == \"train\":\n            self.data = self.data.sample(frac=1).reset_index(drop=True)\n\n    def data_generation(self, index):\n        if self.data_type == \"KE\":\n            X, y = self.generate_all_specs(index)\n        elif self.data_type == \"E\" or self.data_type == \"K\":\n            X, y = self.generate_specs(index)\n        elif self.data_type == \"R\":\n            X, y = self.generate_raw(index)\n        elif self.data_type in [\"ER\", \"KR\"]:\n            X1, y = self.generate_specs(index)\n            X2, y = self.generate_raw(index)\n            X = (X1, X2)\n        elif self.data_type in [\"KER\"]:\n            X1, y = self.generate_all_specs(index)\n            X2, y = self.generate_raw(index)\n            X = (X1, X2)\n        return X, y\n\n    def generate_all_specs(self, index):\n        X = np.zeros((512, 512, 3), dtype=\"float32\")\n        y = np.zeros((6,), dtype=\"float32\")\n\n        row = self.data.iloc[index]\n        if self.mode == \"test\":\n            offset = 0\n        else:\n            offset = int(row.offset / 2)\n\n        eeg = self.eeg_specs[row.eeg_id]\n        spec = self.specs[row.spec_id]\n\n        imgs = [\n            spec[offset : offset + 300, k * 100 : (k + 1) * 100].T for k in [0, 2, 1, 3]\n        ]  # to match kaggle with eeg\n        img = np.stack(imgs, axis=-1)\n        # LOG TRANSFORM SPECTROGRAM\n        img = np.clip(img, np.exp(-4), np.exp(8))\n        img = np.log(img)\n\n        # STANDARDIZE PER IMAGE\n        img = np.nan_to_num(img, nan=0.0)\n\n        mn = img.flatten().min()\n        mx = img.flatten().max()\n        ep = 1e-5\n        img = 255 * (img - mn) / (mx - mn + ep)\n\n        X[0_0 + 56 : 100 + 56, :256, 0] = img[:, 22:-22, 0]  # LL_k\n        X[100 + 56 : 200 + 56, :256, 0] = img[:, 22:-22, 2]  # RL_k\n        X[0_0 + 56 : 100 + 56, :256, 1] = img[:, 22:-22, 1]  # LP_k\n        X[100 + 56 : 200 + 56, :256, 1] = img[:, 22:-22, 3]  # RP_k\n        X[0_0 + 56 : 100 + 56, :256, 2] = img[:, 22:-22, 2]  # RL_k\n        X[100 + 56 : 200 + 56, :256, 2] = img[:, 22:-22, 1]  # LP_k\n\n        X[0_0 + 56 : 100 + 56, 256:, 0] = img[:, 22:-22, 0]  # LL_k\n        X[100 + 56 : 200 + 56, 256:, 0] = img[:, 22:-22, 2]  # RL_k\n        X[0_0 + 56 : 100 + 56, 256:, 1] = img[:, 22:-22, 1]  # LP_k\n        X[100 + 56 : 200 + 56, 256:, 1] = img[:, 22:-22, 3]  # RP_K\n\n        # EEG\n        img = eeg\n        mn = img.flatten().min()\n        mx = img.flatten().max()\n        ep = 1e-5\n        img = 255 * (img - mn) / (mx - mn + ep)\n        X[200 + 56 : 300 + 56, :256, 0] = img[:, 22:-22, 0]  # LL_e\n        X[300 + 56 : 400 + 56, :256, 0] = img[:, 22:-22, 2]  # RL_e\n        X[200 + 56 : 300 + 56, :256, 1] = img[:, 22:-22, 1]  # LP_e\n        X[300 + 56 : 400 + 56, :256, 1] = img[:, 22:-22, 3]  # RP_e\n        X[200 + 56 : 300 + 56, :256, 2] = img[:, 22:-22, 2]  # RL_e\n        X[300 + 56 : 400 + 56, :256, 2] = img[:, 22:-22, 1]  # LP_e\n\n        X[200 + 56 : 300 + 56, 256:, 0] = img[:, 22:-22, 0]  # LL_e\n        X[300 + 56 : 400 + 56, 256:, 0] = img[:, 22:-22, 2]  # RL_e\n        X[200 + 56 : 300 + 56, 256:, 1] = img[:, 22:-22, 1]  # LP_e\n        X[300 + 56 : 400 + 56, 256:, 1] = img[:, 22:-22, 3]  # RP_e\n\n        if self.mode != \"test\":\n            y[:] = row[TARGETS]\n\n        return X, y\n\n    def generate_specs(self, index):\n        X = np.zeros((512, 512, 3), dtype=\"float32\")\n        y = np.zeros((6,), dtype=\"float32\")\n\n        row = self.data.iloc[index]\n        if self.mode == \"test\":\n            offset = 0\n        else:\n            offset = int(row.offset / 2)\n\n        if self.data_type in [\"E\", \"ER\"]:\n            img = self.eeg_specs[row.eeg_id]\n        elif self.data_type in [\"K\", \"KR\"]:\n            spec = self.specs[row.spec_id]\n            imgs = [\n                spec[offset : offset + 300, k * 100 : (k + 1) * 100].T\n                for k in [0, 2, 1, 3]\n            ]  # to match kaggle with eeg\n            img = np.stack(imgs, axis=-1)\n            # LOG TRANSFORM SPECTROGRAM\n            img = np.clip(img, np.exp(-4), np.exp(8))\n            img = np.log(img)\n\n            # STANDARDIZE PER IMAGE\n            img = np.nan_to_num(img, nan=0.0)\n\n        mn = img.flatten().min()\n        mx = img.flatten().max()\n        ep = 1e-5\n        img = 255 * (img - mn) / (mx - mn + ep)\n\n        X[0_0 + 56 : 100 + 56, :256, 0] = img[:, 22:-22, 0]\n        X[100 + 56 : 200 + 56, :256, 0] = img[:, 22:-22, 2]\n        X[0_0 + 56 : 100 + 56, :256, 1] = img[:, 22:-22, 1]\n        X[100 + 56 : 200 + 56, :256, 1] = img[:, 22:-22, 3]\n        X[0_0 + 56 : 100 + 56, :256, 2] = img[:, 22:-22, 2]\n        X[100 + 56 : 200 + 56, :256, 2] = img[:, 22:-22, 1]\n\n        X[0_0 + 56 : 100 + 56, 256:, 0] = img[:, 22:-22, 0]\n        X[100 + 56 : 200 + 56, 256:, 0] = img[:, 22:-22, 1]\n        X[0_0 + 56 : 100 + 56, 256:, 1] = img[:, 22:-22, 2]\n        X[100 + 56 : 200 + 56, 256:, 1] = img[:, 22:-22, 3]\n\n        X[200 + 56 : 300 + 56, :256, 0] = img[:, 22:-22, 0]\n        X[300 + 56 : 400 + 56, :256, 0] = img[:, 22:-22, 1]\n        X[200 + 56 : 300 + 56, :256, 1] = img[:, 22:-22, 2]\n        X[300 + 56 : 400 + 56, :256, 1] = img[:, 22:-22, 3]\n        X[200 + 56 : 300 + 56, :256, 2] = img[:, 22:-22, 3]\n        X[300 + 56 : 400 + 56, :256, 2] = img[:, 22:-22, 2]\n\n        X[200 + 56 : 300 + 56, 256:, 0] = img[:, 22:-22, 0]\n        X[300 + 56 : 400 + 56, 256:, 0] = img[:, 22:-22, 2]\n        X[200 + 56 : 300 + 56, 256:, 1] = img[:, 22:-22, 1]\n        X[300 + 56 : 400 + 56, 256:, 1] = img[:, 22:-22, 3]\n\n        if self.mode != \"test\":\n            y[:] = row[TARGETS]\n\n        return X, y\n\n    def generate_raw(self, index):\n        if USE_PROCESSED and self.mode != \"test\":\n            X = np.zeros((2_000, 8), dtype=\"float32\")\n            y = np.zeros((6,), dtype=\"float32\")\n            row = self.data.iloc[index]\n            X = self.raw_eegs[row.eeg_id]\n            y[:] = row[TARGETS]\n            return X, y\n\n        X = np.zeros((10_000, 8), dtype=\"float32\")\n        y = np.zeros((6,), dtype=\"float32\")\n\n        row = self.data.iloc[index]\n        eeg = self.raw_eegs[row.eeg_id]\n\n        # FEATURE ENGINEER\n        X[:, 0] = eeg[:, FEAT2IDX[\"Fp1\"]] - eeg[:, FEAT2IDX[\"T3\"]]\n        X[:, 1] = eeg[:, FEAT2IDX[\"T3\"]] - eeg[:, FEAT2IDX[\"O1\"]]\n\n        X[:, 2] = eeg[:, FEAT2IDX[\"Fp1\"]] - eeg[:, FEAT2IDX[\"C3\"]]\n        X[:, 3] = eeg[:, FEAT2IDX[\"C3\"]] - eeg[:, FEAT2IDX[\"O1\"]]\n\n        X[:, 4] = eeg[:, FEAT2IDX[\"Fp2\"]] - eeg[:, FEAT2IDX[\"C4\"]]\n        X[:, 5] = eeg[:, FEAT2IDX[\"C4\"]] - eeg[:, FEAT2IDX[\"O2\"]]\n\n        X[:, 6] = eeg[:, FEAT2IDX[\"Fp2\"]] - eeg[:, FEAT2IDX[\"T4\"]]\n        X[:, 7] = eeg[:, FEAT2IDX[\"T4\"]] - eeg[:, FEAT2IDX[\"O2\"]]\n\n        # STANDARDIZE\n        X = np.clip(X, -1024, 1024)\n        X = np.nan_to_num(X, nan=0) / 32.0\n\n        # BUTTER LOW-PASS FILTER\n        X = self.butter_lowpass_filter(X)\n        # Downsample\n        X = X[::5, :]\n\n        if self.mode != \"test\":\n            y[:] = row[TARGETS]\n\n        return X, y\n\n    def butter_lowpass_filter(self, data, cutoff_freq=20, sampling_rate=200, order=4):\n        nyquist = 0.5 * sampling_rate\n        normal_cutoff = cutoff_freq / nyquist\n        b, a = butter(order, normal_cutoff, btype=\"low\", analog=False)\n        filtered_data = lfilter(b, a, data, axis=0)\n        return filtered_data\n\n    def resize(self, img, size):\n        composition = albu.Compose([albu.Resize(size[0], size[1])])\n        return composition(image=img)[\"image\"]\n\n    def augmentation(self, img):\n        composition = albu.Compose([albu.HorizontalFlip(p=0.4)])\n        return composition(image=img)[\"image\"]\n\n\ndef spectrogram_from_eeg(parquet_path):\n\n    # LOAD MIDDLE 50 SECONDS OF EEG SERIES\n    eeg = pd.read_parquet(parquet_path)\n    middle = (len(eeg) - 10_000) // 2\n    eeg = eeg.iloc[middle : middle + 10_000]\n\n    # VARIABLE TO HOLD SPECTROGRAM\n    img = np.zeros((100, 300, 4), dtype=\"float32\")\n\n    for k in range(4):\n        COLS = FEATS[k]\n\n        for kk in range(4):\n            # FILL NANS\n            x1 = eeg[COLS[kk]].values\n            x2 = eeg[COLS[kk + 1]].values\n            m = np.nanmean(x1)\n            if np.isnan(x1).mean() < 1:\n                x1 = np.nan_to_num(x1, nan=m)\n            else:\n                x1[:] = 0\n            m = np.nanmean(x2)\n            if np.isnan(x2).mean() < 1:\n                x2 = np.nan_to_num(x2, nan=m)\n            else:\n                x2[:] = 0\n\n            # COMPUTE PAIR DIFFERENCES\n            x = x1 - x2\n\n            # RAW SPECTROGRAM\n            mel_spec = librosa.feature.melspectrogram(\n                y=x,\n                sr=200,\n                hop_length=len(x) // 300,\n                n_fft=1024,\n                n_mels=100,\n                fmin=0,\n                fmax=20,\n                win_length=128,\n            )\n\n            # LOG TRANSFORM\n            width = (mel_spec.shape[1] // 30) * 30\n            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).astype(np.float32)[\n                :, :width\n            ]\n            img[:, :, k] += mel_spec_db\n\n        # AVERAGE THE 4 MONTAGE DIFFERENCES\n        img[:, :, k] /= 4.0\n\n    return img\n\n\ndef eeg_from_parquet(parquet_path):\n\n    eeg = pd.read_parquet(parquet_path, columns=FEATS2)\n    rows = len(eeg)\n    offset = (rows - 10_000) // 2\n    eeg = eeg.iloc[offset : offset + 10_000]\n    data = np.zeros((10_000, len(FEATS2)))\n    for j, col in enumerate(FEATS2):\n\n        # FILL NAN\n        x = eeg[col].values.astype(\"float32\")\n        m = np.nanmean(x)\n        if np.isnan(x).mean() < 1:\n            x = np.nan_to_num(x, nan=m)\n        else:\n            x[:] = 0\n\n        data[:, j] = x\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.245505Z","iopub.status.idle":"2024-03-12T13:50:56.245868Z","shell.execute_reply.started":"2024-03-12T13:50:56.245674Z","shell.execute_reply":"2024-03-12T13:50:56.245708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MODEL AND UTILITY FUNCTIONS","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import (\n    Input,\n    Dense,\n    Multiply,\n    Add,\n    Conv1D,\n    Concatenate,\n    LayerNormalization,\n)\n\n\ndef build_model():\n    K.clear_session()\n    with strategy.scope():\n        if DATA_TYPE in [\"R\"]:\n            model = build_wave_model()\n        elif DATA_TYPE in [\"K\", \"E\", \"KE\"]:\n            model = build_spec_model()\n        elif DATA_TYPE in [\"KR\", \"ER\", \"KER\"]:\n            model = build_hybrid_model()\n    return model\n\n\ndef build_spec_model(hybrid=False):\n    inp = tf.keras.layers.Input((512, 512, 3))\n    base_model = load_model(f\"{LOAD_BACKBONE_FROM}\")\n    x = base_model(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    if not hybrid:\n        x = tf.keras.layers.Dense(6, activation=\"softmax\", dtype=\"float32\")(x)\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    loss = tf.keras.losses.KLDivergence()\n    model.compile(loss=loss, optimizer=opt)\n    return model\n\n\ndef wave_block(x, filters, kernel_size, n):\n    dilation_rates = [2**i for i in range(n)]\n    x = Conv1D(filters=filters, kernel_size=1, padding=\"same\")(x)\n    res_x = x\n    for dilation_rate in dilation_rates:\n        tanh_out = Conv1D(\n            filters=filters,\n            kernel_size=kernel_size,\n            padding=\"same\",\n            activation=\"tanh\",\n            dilation_rate=dilation_rate,\n        )(x)\n        sigm_out = Conv1D(\n            filters=filters,\n            kernel_size=kernel_size,\n            padding=\"same\",\n            activation=\"sigmoid\",\n            dilation_rate=dilation_rate,\n        )(x)\n        x = Multiply()([tanh_out, sigm_out])\n        x = Conv1D(filters=filters, kernel_size=1, padding=\"same\")(x)\n        res_x = Add()([res_x, x])\n    return res_x\n\n\ndef build_wave_model(hybrid=False):\n\n    # INPUT\n    inp = tf.keras.Input(shape=(2_000, 8))\n\n    ############\n    # FEATURE EXTRACTION SUB MODEL\n    inp2 = tf.keras.Input(shape=(2_000, 1))\n    x = wave_block(inp2, 8, 4, 6)\n    x = wave_block(x, 16, 4, 6)\n    x = wave_block(x, 32, 4, 6)\n    x = wave_block(x, 64, 4, 6)\n    model2 = tf.keras.Model(inputs=inp2, outputs=x)\n    ###########\n\n    # LEFT TEMPORAL CHAIN\n    x1 = model2(inp[:, :, 0:1])\n    x1 = tf.keras.layers.GlobalAveragePooling1D()(x1)\n    x2 = model2(inp[:, :, 1:2])\n    x2 = tf.keras.layers.GlobalAveragePooling1D()(x2)\n    z1 = tf.keras.layers.Average()([x1, x2])\n\n    # LEFT PARASAGITTAL CHAIN\n    x1 = model2(inp[:, :, 2:3])\n    x1 = tf.keras.layers.GlobalAveragePooling1D()(x1)\n    x2 = model2(inp[:, :, 3:4])\n    x2 = tf.keras.layers.GlobalAveragePooling1D()(x2)\n    z2 = tf.keras.layers.Average()([x1, x2])\n\n    # RIGHT PARASAGITTAL CHAIN\n    x1 = model2(inp[:, :, 4:5])\n    x1 = tf.keras.layers.GlobalAveragePooling1D()(x1)\n    x2 = model2(inp[:, :, 5:6])\n    x2 = tf.keras.layers.GlobalAveragePooling1D()(x2)\n    z3 = tf.keras.layers.Average()([x1, x2])\n\n    # RIGHT TEMPORAL CHAIN\n    x1 = model2(inp[:, :, 6:7])\n    x1 = tf.keras.layers.GlobalAveragePooling1D()(x1)\n    x2 = model2(inp[:, :, 7:8])\n    x2 = tf.keras.layers.GlobalAveragePooling1D()(x2)\n    z4 = tf.keras.layers.Average()([x1, x2])\n\n    # COMBINE CHAINS\n    y = tf.keras.layers.Concatenate()([z1, z2, z3, z4])\n    if not hybrid:\n        y = tf.keras.layers.Dense(64, activation=\"relu\")(y)\n        y = tf.keras.layers.Dense(6, activation=\"softmax\", dtype=\"float32\")(y)\n\n    # COMPILE MODEL\n    model = tf.keras.Model(inputs=inp, outputs=y)\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    loss = tf.keras.losses.KLDivergence()\n    model.compile(loss=loss, optimizer=opt)\n\n    return model\n\n\ndef build_hybrid_model():\n    model_spec = build_spec_model(True)\n    model_wave = build_wave_model(True)\n    inputs = [model_spec.input, model_wave.input]\n    x = [model_spec.output, model_wave.output]\n    x = tf.keras.layers.Concatenate()(x)\n    x = tf.keras.layers.Dense(6, activation=\"softmax\", dtype=\"float32\")(x)\n\n    # COMPILE MODEL\n    model = tf.keras.Model(inputs=inputs, outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    loss = tf.keras.losses.KLDivergence()\n    model.compile(loss=loss, optimizer=opt)\n\n    return model\n\n\ndef score(y_true, y_pred):\n    kl = tf.keras.metrics.KLDivergence()\n    return kl(y_true, y_pred)\n\n\ndef plot_hist(hist):\n    metrics = [\"loss\"]\n    for i, metric in enumerate(metrics):\n        plt.figure(figsize=(10, 4))\n        plt.subplot(1, 2, i + 1)\n        plt.plot(hist[metric])\n        plt.plot(hist[f\"val_{metric}\"])\n        plt.title(f\"{metric}\", size=12)\n        plt.ylabel(f\"{metric}\", size=12)\n        plt.xlabel(\"epoch\", size=12)\n        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n        plt.show()\n\n\ndef dataset(\n    data,\n    mode=\"train\",\n    batch_size=8,\n    data_type=DATA_TYPE,\n    augment=False,\n    specs=None,\n    eeg_specs=None,\n    raw_eegs=None,\n):\n\n    BATCH_SIZE_PER_REPLICA = batch_size\n    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n    gen = DataGenerator(\n        data,\n        mode=mode,\n        data_type=data_type,\n        augment=augment,\n        specs=specs,\n        eeg_specs=eeg_specs,\n        raw_eegs=raw_eegs,\n    )\n    if data_type in [\"K\", \"E\", \"KE\"]:\n        inp = tf.TensorSpec(shape=(512, 512, 3), dtype=tf.float32)\n    elif data_type in [\"KR\", \"ER\", \"KER\"]:\n        inp = (\n            tf.TensorSpec(shape=(512, 512, 3), dtype=tf.float32),\n            tf.TensorSpec(shape=(2000, 8), dtype=tf.float32),\n        )\n    elif data_type in [\"R\"]:\n        inp = tf.TensorSpec(shape=(2000, 8), dtype=tf.float32)\n\n    output_signature = (inp, tf.TensorSpec(shape=(6,), dtype=tf.float32))\n    dataset = tf.data.Dataset.from_generator(\n        generator=gen, output_signature=output_signature\n    ).batch(BATCH_SIZE)\n    return dataset\n\n\ndef reset_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.247458Z","iopub.status.idle":"2024-03-12T13:50:56.247956Z","shell.execute_reply.started":"2024-03-12T13:50:56.247672Z","shell.execute_reply":"2024-03-12T13:50:56.247707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Infer Test and Create Submission CSV\n\nInfer the test data and create a `config.output_path` file.\n","metadata":{}},{"cell_type":"code","source":"if submission:\n    test = pd.read_csv(config.data_test_csv)\n    print(\"Test shape\", test.shape)\n    test.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.249047Z","iopub.status.idle":"2024-03-12T13:50:56.249359Z","shell.execute_reply.started":"2024-03-12T13:50:56.249200Z","shell.execute_reply":"2024-03-12T13:50:56.249215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# READ ALL SPECTROGRAMS\nif submission:\n    files2 = os.listdir(config.data_spectograms_test)\n    print(f\"There are {len(files2)} test spectrogram parquets\")\n\n    spectrograms = {}\n    for i, f in enumerate(files2):\n        if i % 100 == 0:\n            print(i, \", \", end=\"\")\n        tmp = pd.read_parquet(f\"{config.data_spectograms_test}/{f}\")\n        name = int(f.split(\".\")[0])\n        spectrograms[name] = tmp.iloc[:, 1:].values\n\n    # RENAME FOR DATA GENERATOR\n    test = test.rename({\"spectrogram_id\": \"spec_id\"}, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.250452Z","iopub.status.idle":"2024-03-12T13:50:56.250811Z","shell.execute_reply.started":"2024-03-12T13:50:56.250602Z","shell.execute_reply":"2024-03-12T13:50:56.250617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# READ ALL EEG SPECTROGRAMS\nif submission:\n    DISPLAY = 0\n    EEG_IDS2 = test.eeg_id.unique()\n    all_eegs2 = {}\n\n    print(\"Converting Test EEG to Spectrograms...\")\n    print()\n    for i, eeg_id in enumerate(EEG_IDS2):\n\n        # CREATE SPECTROGRAM FROM EEG PARQUET\n        img = spectrogram_from_eeg(f\"{config.data_eeg_test}{eeg_id}.parquet\")\n        all_eegs2[eeg_id] = img","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.252422Z","iopub.status.idle":"2024-03-12T13:50:56.252778Z","shell.execute_reply.started":"2024-03-12T13:50:56.252576Z","shell.execute_reply":"2024-03-12T13:50:56.252590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# READ ALL RAW EEG SIGNALS\nif submission:\n    all_raw_eegs2 = {}\n    EEG_IDS2 = test.eeg_id.unique()\n\n    print(\"Processing Test EEG parquets...\")\n    print()\n    for i, eeg_id in enumerate(EEG_IDS2):\n\n        # SAVE EEG TO PYTHON DICTIONARY OF NUMPY ARRAYS\n        data = eeg_from_parquet(f\"{config.data_eeg_test}{eeg_id}.parquet\")\n        all_raw_eegs2[eeg_id] = data","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.254166Z","iopub.status.idle":"2024-03-12T13:50:56.254469Z","shell.execute_reply.started":"2024-03-12T13:50:56.254319Z","shell.execute_reply":"2024-03-12T13:50:56.254333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission ON TEST with ensemble\nif submission and ENSEMBLE:\n    preds = []\n    params = {\"specs\": spectrograms, \"eeg_specs\": all_eegs2, \"raw_eegs\": all_raw_eegs2}\n    test_dataset_K = dataset(test, data_type=\"K\", mode=\"test\", **params)\n    test_dataset_E = dataset(test, data_type=\"E\", mode=\"test\", **params)\n    test_dataset_R = dataset(test, data_type=\"R\", mode=\"test\", **params)\n    test_dataset_KE = dataset(test, data_type=\"KE\", mode=\"test\", **params)\n    test_dataset_KR = dataset(test, data_type=\"KR\", mode=\"test\", **params)\n    test_dataset_ER = dataset(test, data_type=\"ER\", mode=\"test\", **params)\n    test_dataset_KER = dataset(test, data_type=\"KER\", mode=\"test\", **params)\n\n    # LB SCORE WEIGHTS FOR EACH MODEL\n    lbs = 1 - np.array(LBs)\n    weights = lbs / lbs.sum()\n    model_spec = build_spec_model()\n    model_wave = build_wave_model()\n    model_hybrid = build_hybrid_model()\n\n    for i in range(5):\n        print(f\"Fold {i+1}\")\n\n        model_spec.load_weights(f\"{LOAD_MODELS_FROM}/model_K_{VER_K}_{i}.weights.h5\")\n        pred_K = model_spec.predict(test_dataset_K, verbose=1)\n\n        model_spec.load_weights(f\"{LOAD_MODELS_FROM}/model_E_{VER_E}_{i}.weights.h5\")\n        pred_E = model_spec.predict(test_dataset_E, verbose=1)\n\n        model_wave.load_weights(f\"{LOAD_MODELS_FROM}/model_R_{VER_R}_{i}.weights.h5\")\n        pred_R = model_wave.predict(test_dataset_R, verbose=1)\n\n        model_spec.load_weights(f\"{LOAD_MODELS_FROM}/model_KE_{VER_KE}_{i}.weights.h5\")\n        pred_KE = model_spec.predict(test_dataset_KE, verbose=1)\n\n        model_hybrid.load_weights(\n            f\"{LOAD_MODELS_FROM}/model_KR_{VER_KR}_{i}.weights.h5\"\n        )\n        pred_KR = model_hybrid.predict(test_dataset_KR, verbose=1)\n\n        model_hybrid.load_weights(\n            f\"{LOAD_MODELS_FROM}/model_ER_{VER_ER}_{i}.weights.h5\"\n        )\n        pred_ER = model_hybrid.predict(test_dataset_ER, verbose=1)\n\n        model_hybrid.load_weights(\n            f\"{LOAD_MODELS_FROM}/model_KER_{VER_KER}_{i}.weights.h5\"\n        )\n        pred_KER = model_hybrid.predict(test_dataset_KER, verbose=1)\n\n        pred = np.array([pred_K, pred_E, pred_R, pred_KE, pred_KR, pred_ER, pred_KER])\n        pred = np.average(pred, axis=0, weights=weights)\n        preds.append(pred)\n\n    pred_model4 = np.mean(preds, axis=0)\n    print(\"Test preds shape\", pred_model4.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.255814Z","iopub.status.idle":"2024-03-12T13:50:56.256131Z","shell.execute_reply.started":"2024-03-12T13:50:56.255977Z","shell.execute_reply":"2024-03-12T13:50:56.255992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FREE MEMORY\ndel model_spec, model_wave, model_hybrid\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.257420Z","iopub.status.idle":"2024-03-12T13:50:56.257832Z","shell.execute_reply.started":"2024-03-12T13:50:56.257621Z","shell.execute_reply":"2024-03-12T13:50:56.257638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if submission:\n#     sub4 = pd.DataFrame({\"eeg_id\": test.eeg_id.values})\n#     sub4[TARGETS] = pred_model4\n#     print(\"Submissionn shape\", sub4.shape)\n#     print()\n#     sub4.to_csv(\"submission_model4.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.258821Z","iopub.status.idle":"2024-03-12T13:50:56.259193Z","shell.execute_reply.started":"2024-03-12T13:50:56.259023Z","shell.execute_reply":"2024-03-12T13:50:56.259039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\n# if submission:\n#     print(sub4.iloc[:, -6:].sum(axis=1).to_string())","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.260594Z","iopub.status.idle":"2024-03-12T13:50:56.260936Z","shell.execute_reply.started":"2024-03-12T13:50:56.260767Z","shell.execute_reply":"2024-03-12T13:50:56.260783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_model4","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.262094Z","iopub.status.idle":"2024-03-12T13:50:56.262441Z","shell.execute_reply.started":"2024-03-12T13:50:56.262268Z","shell.execute_reply":"2024-03-12T13:50:56.262285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 5 - WaveNet","metadata":{}},{"cell_type":"markdown","source":"### [CV 0.68 | LB 0.46] DilatedInception WaveNet in PyTorch - Inference\n\n#### Introduction\nAfter joining this competition, I focus on validating how far raw EEG signals can go through many experiments. Finally, I find an simple architecture mixing the concept of **dilation** and **inception**, which can be seen as an extension of [Chris' version](https://www.kaggle.com/code/cdeotte/wavenet-starter-lb-0-52). And, I'm happy to announce that we can achieve CV 0.68 (below 0.7) and LB 0.46 with this architecture. Don't forget to upvote Chris' notebook!\n\n#### About this Notebook\nIn this kernel, I run the inference process with 5-fold models (equally-weighted blending) and obtain LB 0.46. If you're also interested in training part, pleasse see [[LB 0.46] DilatedInception WaveNet - Training](https://www.kaggle.com/code/abaojiang/lb-0-46-dilatedinception-wavenet-training).\n\n#### Acknowledgements\nSpecial thanks to [@cdeotte](https://www.kaggle.com/cdeotte)'s sharing, [WaveNet Starter - [LB 0.52]](https://www.kaggle.com/code/cdeotte/wavenet-starter-lb-0-52).\n\n<a id=\"toc\"></a>\n## Table of Contents\n* [1. Load Data](#load_data)\n* [2. Define Dataset](#dataset)\n* [3. Create Test Loader](#test_loader)\n* [4. Define Model Architecture](#model)\n* [5. Load Models](#load_models)\n* [6. Run Inference](#infer)\n* [7. Submission](#sub)\n\n#### Import Packages","metadata":{}},{"cell_type":"code","source":"import gc\nimport os\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nimport pickle\nimport warnings\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nwarnings.simplefilter(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport yaml\nfrom scipy.signal import butter, lfilter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.263312Z","iopub.status.idle":"2024-03-12T13:50:56.263632Z","shell.execute_reply.started":"2024-03-12T13:50:56.263469Z","shell.execute_reply":"2024-03-12T13:50:56.263485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Define Data Paths and Configuration and Metadata","metadata":{}},{"cell_type":"code","source":"DATA_PATH = Path(config.competition_data_path)\n\nclass CFG:\n    exp_id = \"0311-17-20-55\"\n    model_path = Path(f\"{config.full_path}/dilated-wavenet/0311-17-20-55\")\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    # == Data ==\n    # Chris' 8 channels\n    feats = [\n        \"Fp1\", \"T3\", \"C3\", \"O1\",\n        \"Fp2\", \"C4\", \"T4\", \"O2\"\n    ]\n    cast_eegs = True\n    dataset = {\n        \"eeg\": {\n            \"n_feats\": 8,\n            \"apply_chris_magic_ch8\": True,\n            \"normalize\": True,\n            \"apply_butter_lowpass_filter\": True,\n            \"apply_mu_law_encoding\": False,\n            \"downsample\": 5\n        }\n    }\n    \n    # == Data Loader ==\n    batch_size = 32\n    \n    \nN_CLASSES = 6\nTGT_VOTE_COLS = [\n    \"seizure_vote\", \"lpd_vote\", \"gpd_vote\", \"lrda_vote\",\n    \"grda_vote\", \"other_vote\"\n]\nEEG_FREQ = 200  # Hz\nEEG_WLEN = 50  # sec\nEEG_PTS = int(EEG_FREQ * EEG_WLEN)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.265323Z","iopub.status.idle":"2024-03-12T13:50:56.265831Z","shell.execute_reply.started":"2024-03-12T13:50:56.265550Z","shell.execute_reply":"2024-03-12T13:50:56.265589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load Data\n\nNote test data will be replaced with **hidden test set** during rerun.","metadata":{}},{"cell_type":"code","source":"def _get_eeg_window(file: Path) -> np.ndarray:\n    \"\"\"Return cropped EEG window.\n\n    Default setting is to return the middle 50-sec window.\n\n    Args:\n        file: EEG file path\n        test: if True, there's no need to truncate EEGs\n\n    Returns:\n        eeg_win: cropped EEG window \n    \"\"\"\n    eeg = pd.read_parquet(file, columns=CFG.feats)\n    n_pts = len(eeg)\n    offset = (n_pts - EEG_PTS) // 2\n    eeg = eeg.iloc[offset:offset + EEG_PTS]\n    \n    eeg_win = np.zeros((EEG_PTS, len(CFG.feats)))\n    for j, col in enumerate(CFG.feats):\n        if CFG.cast_eegs:\n            eeg_raw = eeg[col].values.astype(\"float32\")\n        else:\n            eeg_raw = eeg[col].values \n\n        # Fill missing values\n        mean = np.nanmean(eeg_raw)\n        if np.isnan(eeg_raw).mean() < 1:\n            eeg_raw = np.nan_to_num(eeg_raw, nan=mean)\n        else: \n            # All missing\n            eeg_raw[:] = 0\n        eeg_win[:, j] = eeg_raw \n        \n    return eeg_win ","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.267305Z","iopub.status.idle":"2024-03-12T13:50:56.267771Z","shell.execute_reply.started":"2024-03-12T13:50:56.267522Z","shell.execute_reply":"2024-03-12T13:50:56.267544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(DATA_PATH / \"test.csv\")\nprint(f\"Test data shape | {test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.268862Z","iopub.status.idle":"2024-03-12T13:50:56.269296Z","shell.execute_reply.started":"2024-03-12T13:50:56.269068Z","shell.execute_reply":"2024-03-12T13:50:56.269089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uniq_eeg_ids = test[\"eeg_id\"].unique()\nn_uniq_eeg_ids = len(uniq_eeg_ids)\n\nall_eegs = {}\nfor i, eeg_id in tqdm(enumerate(uniq_eeg_ids), total=n_uniq_eeg_ids):\n    eeg_win = _get_eeg_window(DATA_PATH / \"test_eegs\" / f\"{eeg_id}.parquet\")\n    all_eegs[eeg_id] = eeg_win\n\nprint(f\"Demo EEG shape | {list(all_eegs.values())[0].shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.270397Z","iopub.status.idle":"2024-03-12T13:50:56.270898Z","shell.execute_reply.started":"2024-03-12T13:50:56.270638Z","shell.execute_reply":"2024-03-12T13:50:56.270661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Define Dataset","metadata":{}},{"cell_type":"code","source":"class _EEGTransformer(object):\n    \"\"\"Data transformer for raw EEG signals.\"\"\"\n\n    FEAT2CODE = {f: i for i, f in enumerate(CFG.feats)}\n\n    def __init__(\n        self,\n        n_feats: int,\n        apply_chris_magic_ch8: bool = True,\n        normalize: bool = True,\n        apply_butter_lowpass_filter: bool = True,\n        apply_mu_law_encoding: bool = False,\n        downsample: Optional[int] = None,\n    ) -> None:\n        self.n_feats = n_feats\n        self.apply_chris_magic_ch8 = apply_chris_magic_ch8\n        self.normalize = normalize\n        self.apply_butter_lowpass_filter = apply_butter_lowpass_filter\n        self.apply_mu_law_encoding = apply_mu_law_encoding\n        self.downsample = downsample\n\n    def transform(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Apply transformation on raw EEG signals.\n        \n        Args:\n            x: raw EEG signals, with shape (L, C)\n\n        Return:\n            x_: transformed EEG signals\n        \"\"\"\n        x_ = x.copy()\n        if self.apply_chris_magic_ch8:\n            x_ = self._apply_chris_magic_ch8(x_)\n\n        if self.normalize:\n            x_ = np.clip(x_, -1024, 1024)\n            x_ = np.nan_to_num(x_, nan=0) / 32.0\n\n        if self.apply_butter_lowpass_filter:\n            x_ = self._butter_lowpass_filter(x_) \n\n        if self.apply_mu_law_encoding:\n            x_ = self._quantize_data(x_, 1)\n\n        if self.downsample is not None:\n            x_ = x_[::self.downsample, :]\n\n        return x_\n\n    def _apply_chris_magic_ch8(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Generate features based on Chris' magic formula.\"\"\" \n        x_tmp = np.zeros((EEG_PTS, self.n_feats), dtype=\"float32\")\n\n        # Generate features\n        x_tmp[:, 0] = x[:, self.FEAT2CODE[\"Fp1\"]] - x[:, self.FEAT2CODE[\"T3\"]]\n        x_tmp[:, 1] = x[:, self.FEAT2CODE[\"T3\"]] - x[:, self.FEAT2CODE[\"O1\"]]\n        \n        x_tmp[:, 2] = x[:, self.FEAT2CODE[\"Fp1\"]] - x[:, self.FEAT2CODE[\"C3\"]]\n        x_tmp[:, 3] = x[:, self.FEAT2CODE[\"C3\"]] - x[:, self.FEAT2CODE[\"O1\"]]\n        \n        x_tmp[:, 4] = x[:, self.FEAT2CODE[\"Fp2\"]] - x[:, self.FEAT2CODE[\"C4\"]]\n        x_tmp[:, 5] = x[:, self.FEAT2CODE[\"C4\"]] - x[:, self.FEAT2CODE[\"O2\"]]\n        \n        x_tmp[:, 6] = x[:, self.FEAT2CODE[\"Fp2\"]] - x[:, self.FEAT2CODE[\"T4\"]]\n        x_tmp[:, 7] = x[:, self.FEAT2CODE[\"T4\"]] - x[:, self.FEAT2CODE[\"O2\"]]\n\n        return x_tmp\n\n    def _butter_lowpass_filter(self, data, cutoff_freq=20, sampling_rate=200, order=4):\n        nyquist = 0.5 * sampling_rate\n        normal_cutoff = cutoff_freq / nyquist\n        b, a = butter(order, normal_cutoff, btype=\"low\", analog=False)\n        filtered_data = lfilter(b, a, data, axis=0)\n\n        return filtered_data\n                \n    def _quantize_data(self, data, classes):\n        mu_x = self._mu_law_encoding(data, classes)\n        \n        return mu_x\n\n    def _mu_law_encoding(self, data, mu):\n        mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n\n        return mu_x","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.272308Z","iopub.status.idle":"2024-03-12T13:50:56.272795Z","shell.execute_reply.started":"2024-03-12T13:50:56.272548Z","shell.execute_reply":"2024-03-12T13:50:56.272570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    \"\"\"Dataset for pure raw EEG signals.\n\n    Args:\n        data: processed data\n        split: data split\n\n    Attributes:\n        _n_samples: number of samples\n        _infer: if True, the dataset is constructed for inference\n            *Note: Ground truth is not provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Dict[str,  Any],\n        split: str,\n        **dataset_cfg: Any,\n    ) -> None:\n        self.metadata = data[\"meta\"]\n        self.all_eegs = data[\"eeg\"]\n        self.dataset_cfg = dataset_cfg\n\n        # Raw EEG data transformer\n        self.eeg_params = dataset_cfg[\"eeg\"]\n        self.eeg_trafo = _EEGTransformer(**self.eeg_params)\n\n        self._set_n_samples()\n        self._infer = True if split == \"test\" else False\n\n        self._stream_X = True if self.all_eegs is None else False\n        self._X, self._y = self._transform()\n\n    def _set_n_samples(self) -> None:\n        assert len(self.metadata) == self.metadata[\"eeg_id\"].nunique()\n        self._n_samples = len(self.metadata)\n\n    def _transform(self) -> Tuple[Optional[np.ndarray], np.ndarray]:\n        \"\"\"Transform feature and target matrices.\"\"\"\n        if self.eeg_params[\"downsample\"] is not None:\n            eeg_len = int(EEG_PTS / self.eeg_params[\"downsample\"])\n        else:\n            eeg_len = int(EEG_PTS)\n        if not self._stream_X:\n            X = np.zeros((self._n_samples, eeg_len, self.eeg_params[\"n_feats\"]), dtype=\"float32\")\n        else:\n            X = None\n        y = np.zeros((self._n_samples, N_CLASSES), dtype=\"float32\") if not self._infer else None\n\n        for i, row in tqdm(self.metadata.iterrows(), total=len(self.metadata)):\n            # Process raw EEG signals\n            if not self._stream_X:\n                # Retrieve raw EEG signals\n                eeg = self.all_eegs[row[\"eeg_id\"]]\n\n                # Apply EEG transformer\n                x = self.eeg_trafo.transform(eeg)\n\n                X[i] = x\n\n            if not self._infer:\n                y[i] = row[TGT_VOTE_COLS] \n\n        return X, y\n\n    def __len__(self) -> int:\n        return self._n_samples\n\n    def __getitem__(self, idx: int) -> Dict[str, Tensor]:\n        if self._X is None:\n            # Load data here...\n#             x = np.load(...)\n#             x = self.eeg_trafo.transform(x)\n            pass\n        else:\n            x = self._X[idx, ...]\n        data_sample = {\"x\": torch.tensor(x, dtype=torch.float32)}\n        if not self._infer:\n            data_sample[\"y\"] = torch.tensor(self._y[idx, :], dtype=torch.float32)\n\n        return data_sample","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.274757Z","iopub.status.idle":"2024-03-12T13:50:56.275142Z","shell.execute_reply.started":"2024-03-12T13:50:56.274957Z","shell.execute_reply":"2024-03-12T13:50:56.274975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"test_loader\"></a>\n## 3. Create Test Loader\n[**<span style=\"color:#FEF1FE; background-color:#535d70;border-radius: 5px; padding: 2px\">Go to Table of Content</span>**](#toc)","metadata":{}},{"cell_type":"code","source":"test_data = {\"meta\": test, \"eeg\": all_eegs}\ntest_loader = DataLoader(\n    EEGDataset(test_data, \"test\", **CFG.dataset),\n    batch_size=CFG.batch_size,\n    shuffle=False,\n    num_workers=0\n)\nprint(f\"There are {len(test_loader.dataset)} test samples to infer.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.276243Z","iopub.status.idle":"2024-03-12T13:50:56.276571Z","shell.execute_reply.started":"2024-03-12T13:50:56.276408Z","shell.execute_reply":"2024-03-12T13:50:56.276423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Define Model Architecture\n\n[![Screenshot-2024-02-19-at-1-11-40-PM.png](https://i.postimg.cc/MKp8xVVV/Screenshot-2024-02-19-at-1-11-40-PM.png)](https://postimg.cc/7bdRnCBZ)","metadata":{}},{"cell_type":"code","source":"class _WaveBlock(nn.Module):\n    \"\"\"WaveNet block.\n\n    Args:\n        kernel_size: kernel size, pass a list of kernel sizes for\n            inception\n    \"\"\"\n\n    def __init__(\n        self,\n        n_layers: int, \n        in_dim: int,\n        h_dim: int,\n        kernel_size: Union[int, List[int]],\n        conv_module: Optional[Type[nn.Module]] = None,\n    ) -> None:\n        super().__init__()\n\n        self.n_layers = n_layers\n        self.dilation_rates = [2**l for l in range(n_layers)]\n\n        self.in_conv = nn.Conv2d(in_dim, h_dim, kernel_size=(1, 1)) \n        self.gated_tcns = nn.ModuleList()\n        self.skip_convs = nn.ModuleList()\n        for layer in range(n_layers):\n            c_in, c_out = h_dim, h_dim\n            self.gated_tcns.append(\n                _GatedTCN(\n                    in_dim=c_in,\n                    h_dim=c_out,\n                    kernel_size=kernel_size,\n                    dilation_factor=self.dilation_rates[layer],\n                    conv_module=conv_module,\n                )\n            )\n            self.skip_convs.append(nn.Conv2d(h_dim, h_dim, kernel_size=(1, 1)))\n\n        # Initialize parameters\n        nn.init.xavier_uniform_(self.in_conv.weight, gain=nn.init.calculate_gain(\"relu\"))\n        nn.init.zeros_(self.in_conv.bias)\n        for i in range(len(self.skip_convs)):\n            nn.init.xavier_uniform_(self.skip_convs[i].weight, gain=nn.init.calculate_gain(\"relu\"))\n            nn.init.zeros_(self.skip_convs[i].bias)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass.\n        \n        Shape:\n            x: (B, C, N, L), where C denotes in_dim\n            x_skip: (B, C', N, L), where C' denotes h_dim\n        \"\"\"\n        # Input convolution\n        x = self.in_conv(x)\n\n        x_skip = x\n        for layer in range(self.n_layers):\n            x = self.gated_tcns[layer](x)\n            x = self.skip_convs[layer](x)\n\n            # Skip-connection\n            x_skip = x_skip + x \n\n        return x_skip\n\n\nclass _GatedTCN(nn.Module):\n    \"\"\"Gated temporal convolution layer.\n\n    Parameters:\n        conv_module: customized convolution module\n    \"\"\"\n\n    def __init__(\n        self,\n        in_dim: int,\n        h_dim: int,\n        kernel_size: Union[int, List[int]],\n        dilation_factor: int,\n        dropout: Optional[float] = None,\n        conv_module: Optional[Type[nn.Module]] = None,\n    ) -> None:\n        super().__init__()\n\n        # Model blocks\n        if conv_module is None:\n            self.filt = nn.Conv2d(\n                in_channels=in_dim, out_channels=h_dim, kernel_size=(1, kernel_size), dilation=dilation_factor\n            )\n            self.gate = nn.Conv2d(\n                in_channels=in_dim, out_channels=h_dim, kernel_size=(1, kernel_size), dilation=dilation_factor\n            )\n        else:\n            self.filt = conv_module(\n                in_channels=in_dim, out_channels=h_dim, kernel_size=kernel_size, dilation=dilation_factor\n            )\n            self.gate = conv_module(\n                in_channels=in_dim, out_channels=h_dim, kernel_size=kernel_size, dilation=dilation_factor\n            )\n\n        if dropout is not None:\n            self.dropout = nn.Dropout(dropout)\n        else:\n            self.dropout = None\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass.\n\n        Parameters:\n            x: input sequence\n\n        Return:\n            h: output sequence\n\n        Shape:\n            x: (B, C, N, L), where L denotes the input sequence length\n            h: (B, h_dim, N, L')\n        \"\"\"\n        x_filt = F.tanh(self.filt(x))\n        x_gate = F.sigmoid(self.gate(x))\n        h = x_filt * x_gate\n        if self.dropout is not None:\n            h = self.dropout(h)\n\n        return h\n\n\nclass _DilatedInception(nn.Module):\n    \"\"\"Dilated inception layer.\n\n    Note that `out_channels` will be split across #kernels.\n    \"\"\"\n\n    def __init__(\n        self, \n        in_channels: int, \n        out_channels: int, \n        kernel_size: List[int], \n        dilation: int\n    ) -> None:\n        super().__init__()\n\n        # Network parameters\n        n_kernels = len(kernel_size)\n        assert out_channels % n_kernels == 0, \"`out_channels` must be divisible by #kernels.\"\n        h_dim = out_channels // n_kernels\n\n        # Model blocks\n        self.convs = nn.ModuleList()\n        for k in kernel_size:\n            self.convs.append(\n                nn.Conv2d(\n                    in_channels=in_channels, \n                    out_channels=h_dim, \n                    kernel_size=(1, k),\n                    padding=\"same\",\n                    dilation=dilation),\n            )\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass.\n\n        Parameters:\n            x: input sequence\n\n        Return:\n            h: output sequence\n\n        Shape:\n            x: (B, C, N, L), where C = in_channels\n            h: (B, C', N, L'), where C' = out_channels\n        \"\"\"\n        x_convs = []\n        for conv in self.convs:\n            x_conv = conv(x)\n            x_convs.append(x_conv)\n        h = torch.cat(x_convs, dim=1)\n\n        return h","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.278080Z","iopub.status.idle":"2024-03-12T13:50:56.278415Z","shell.execute_reply.started":"2024-03-12T13:50:56.278244Z","shell.execute_reply":"2024-03-12T13:50:56.278260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DilatedInceptionWaveNet(nn.Module):\n    \"\"\"WaveNet architecture with dilated inception conv.\"\"\"\n\n    def __init__(self,) -> None:\n        super().__init__()\n\n        kernel_size = [2, 3, 6, 7]\n\n        # Model blocks \n        self.wave_module = nn.Sequential(\n            _WaveBlock(12, 1, 16, kernel_size, _DilatedInception),\n            _WaveBlock(8, 16, 32, kernel_size, _DilatedInception),\n            _WaveBlock(4, 32, 64, kernel_size, _DilatedInception),\n            _WaveBlock(1, 64, 64, kernel_size, _DilatedInception),\n        )\n        self.output = nn.Sequential(\n            nn.Linear(64 * 4, 64),\n            nn.ReLU(),\n            nn.Linear(64, N_CLASSES)\n        ) \n\n    def forward(self, inputs: Dict[str, Tensor]) -> Tensor:\n        \"\"\"Forward pass.\n\n        Shape:\n            x: (B, L, C)\n        \"\"\"\n        x = inputs[\"x\"]\n        bs, length, in_dim = x.shape\n        x = x.transpose(1, 2).unsqueeze(dim=2)  # (B, C, N, L), N is redundant\n\n        x_ll_1 = self.wave_module(x[:, 0:1, :])\n        x_ll_2 = self.wave_module(x[:, 1:2, :])\n        x_ll = (F.adaptive_avg_pool2d(x_ll_1, (1, 1)) + F.adaptive_avg_pool2d(x_ll_2, (1, 1))) / 2\n\n        x_rl_1 = self.wave_module(x[:, 2:3, :])\n        x_rl_2 = self.wave_module(x[:, 3:4, :])\n        x_rl = (F.adaptive_avg_pool2d(x_rl_1, (1, 1)) + F.adaptive_avg_pool2d(x_rl_2, (1, 1))) / 2\n\n        x_lp_1 = self.wave_module(x[:, 4:5, :])\n        x_lp_2 = self.wave_module(x[:, 5:6, :])\n        x_lp = (F.adaptive_avg_pool2d(x_lp_1, (1, 1)) + F.adaptive_avg_pool2d(x_lp_2, (1, 1))) / 2\n\n        x_rp_1 = self.wave_module(x[:, 6:7, :])\n        x_rp_2 = self.wave_module(x[:, 7:8, :])\n        x_rp = (F.adaptive_avg_pool2d(x_rp_1, (1, 1)) + F.adaptive_avg_pool2d(x_rp_2, (1, 1))) / 2\n\n        x = torch.cat([x_ll, x_rl, x_lp, x_rp], axis=1).reshape(bs, -1)\n        output = self.output(x)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.279789Z","iopub.status.idle":"2024-03-12T13:50:56.280118Z","shell.execute_reply.started":"2024-03-12T13:50:56.279953Z","shell.execute_reply":"2024-03-12T13:50:56.279969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. Load Models","metadata":{}},{"cell_type":"code","source":"models = []\nfor fold, file in enumerate(sorted(CFG.model_path.glob(\"./*.pth\"))):\n    print(f\"Load model from {file}...\")\n    fold_model = DilatedInceptionWaveNet()\n    fold_model.load_state_dict(torch.load(file, map_location=CFG.device))\n    fold_model = fold_model.to(CFG.device)\n    models.append(fold_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.281875Z","iopub.status.idle":"2024-03-12T13:50:56.282216Z","shell.execute_reply.started":"2024-03-12T13:50:56.282051Z","shell.execute_reply":"2024-03-12T13:50:56.282067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6. Run Inference","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef _infer(inputs: Dict[str, Tensor], models: List[nn.Module]) -> Tensor:\n    n_models = len(models)\n\n    for i, model in enumerate(models):\n        model.eval()\n        y_pred_fold = F.softmax(model(inputs)) / n_models    # (B, N_CLASSES)\n        \n        if i == 0:\n            y_pred = y_pred_fold\n        else:\n            y_pred += y_pred_fold\n        \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.283657Z","iopub.status.idle":"2024-03-12T13:50:56.284062Z","shell.execute_reply.started":"2024-03-12T13:50:56.283884Z","shell.execute_reply":"2024-03-12T13:50:56.283902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = []\nfor i, batch_data in enumerate(test_loader):\n    batch_data[\"x\"] = batch_data[\"x\"].to(CFG.device)\n    y_pred = _infer(batch_data, models)\n    y_preds.append(y_pred.detach().cpu().numpy())\ny_preds = np.vstack(y_preds)\nprint(f\"Sum of row 0 in y_preds {np.sum(y_preds[0, :])}.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.285034Z","iopub.status.idle":"2024-03-12T13:50:56.285359Z","shell.execute_reply.started":"2024-03-12T13:50:56.285192Z","shell.execute_reply":"2024-03-12T13:50:56.285208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7. Submission","metadata":{}},{"cell_type":"code","source":"pred_model5 = y_pred","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.286606Z","iopub.status.idle":"2024-03-12T13:50:56.286964Z","shell.execute_reply.started":"2024-03-12T13:50:56.286798Z","shell.execute_reply":"2024-03-12T13:50:56.286814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if submission:\n#     sub5 = pd.DataFrame({\"eeg_id\": test.eeg_id.values})\n#     sub5[TARGETS] = pred_model5\n#     print(\"Submissionn shape\", sub5.shape)\n#     print()\n#     print(sub5.head().to_string())\n# #     sub5.to_csv(\"submission_model5.csv\", index=False)\n#     sub5.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.288498Z","iopub.status.idle":"2024-03-12T13:50:56.288839Z","shell.execute_reply.started":"2024-03-12T13:50:56.288652Z","shell.execute_reply":"2024-03-12T13:50:56.288666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\n# if submission:\n#     print(sub5.iloc[:, -6:].sum(axis=1).to_string())","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.290018Z","iopub.status.idle":"2024-03-12T13:50:56.290365Z","shell.execute_reply.started":"2024-03-12T13:50:56.290194Z","shell.execute_reply":"2024-03-12T13:50:56.290210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 6 - CatBoost Starter","metadata":{}},{"cell_type":"markdown","source":"## CatBoost Starter for Brain Comp\nThis is a CatBoost starter notebook for Kaggle's brain comp. We use only spectrogram features. (The model does not use eeg features yet). We can improve the CV and LB score by engineering more (spectrogram and/or eeg) features and we can tune the CatBoost model (and/or use other ML DL models). Discussion about this starter is [here][2].\n\nIn this notebook, we also compare five CV scores. Kaggle's sample submission uses equal predictions of 1/6 for all targets and achieves CV 1.46, LB 1.09. The best public notebook (on Jan 12th) [here][1] uses train means and achieves CV 1.26 LB 0.97. Our CatBoost model version 1 achieves CV 1.01 LB 0.81. Our CatBoost model version 2 achieves CV 0.82 LB 0.67. Then version 3 adds features from **EEG spectrograms** and achieves CV 0.74, wow! Let's see what LB is...\n\n### Exciting UPDATE!\nVersion 3 of this notebook trains using **both** Kaggle spectrograms and my new **EEG spectrograms** from my Kaggle dataset [here][3] (which were created from my spectrogram starter [here][4]). We boost the CV score and (most likely) LB score by almost `+0.10`, wow! \n\n#### Version Notes\n* Version 1 - Uses spectrogram features from 10 minute window `means`. Achieves CV 1.01, LB 0.81\n* Version 2 - Uses spectrogram features from 10 minute and 20 second `means` and `mins`. Achieves CV 0.82, LB 0.67\n* Version 3 - Uses Kaggle spectrogrms **plus EEG spectrograms**. Achieves 0.74, LB to be determined...\n\n[1]: https://www.kaggle.com/code/seshurajup/eda-train-csv\n[2]: https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/467576\n[3]: https://www.kaggle.com/datasets/cdeotte/brain-eeg-spectrograms\n[4]: https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg","metadata":{}},{"cell_type":"markdown","source":"### Load Libraries","metadata":{}},{"cell_type":"code","source":"import os, gc\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\n\nVER = 3","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.291878Z","iopub.status.idle":"2024-03-12T13:50:56.292205Z","shell.execute_reply.started":"2024-03-12T13:50:56.292045Z","shell.execute_reply":"2024-03-12T13:50:56.292061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Train Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(config.data_train_csv)\nTARGETS = df.columns[-6:]\nprint(\"Train shape:\", df.shape)\nprint(\"Targets\", list(TARGETS))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.293443Z","iopub.status.idle":"2024-03-12T13:50:56.293842Z","shell.execute_reply.started":"2024-03-12T13:50:56.293618Z","shell.execute_reply":"2024-03-12T13:50:56.293635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Non-Overlapping Eeg Id Train Data\nThe competition data description says that test data does not have multiple crops from the same `eeg_id`. Therefore we will train and validate using only 1 crop per `eeg_id`. There is a discussion about this [here][1].\n\n[1]: https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/467021","metadata":{}},{"cell_type":"code","source":"train = hp.preprocess_eeg_data(train_df, TARGETS)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.295134Z","iopub.status.idle":"2024-03-12T13:50:56.295491Z","shell.execute_reply.started":"2024-03-12T13:50:56.295311Z","shell.execute_reply":"2024-03-12T13:50:56.295328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Infer Test and Create Submission CSV\nBelow we use our 5 CatBoost fold models to infer the test data and create a `submission.csv` file.","metadata":{}},{"cell_type":"code","source":"import pywt, librosa\n\nUSE_WAVELET = None\n\nNAMES = [\"LL\", \"LP\", \"RP\", \"RR\"]\n\nFEATS = [\n    [\"Fp1\", \"F7\", \"T3\", \"T5\", \"O1\"],\n    [\"Fp1\", \"F3\", \"C3\", \"P3\", \"O1\"],\n    [\"Fp2\", \"F8\", \"T4\", \"T6\", \"O2\"],\n    [\"Fp2\", \"F4\", \"C4\", \"P4\", \"O2\"],\n]\n\n\n# DENOISE FUNCTION\ndef maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\n\ndef denoise(x, wavelet=\"haar\", level=1):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1 / 0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2 * np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode=\"hard\") for i in coeff[1:])\n\n    ret = pywt.waverec(coeff, wavelet, mode=\"per\")\n\n    return ret\n\n\ndef spectrogram_from_eeg(parquet_path, display=False):\n\n    # LOAD MIDDLE 50 SECONDS OF EEG SERIES\n    eeg = pd.read_parquet(parquet_path)\n    middle = (len(eeg) - 10_000) // 2\n    eeg = eeg.iloc[middle : middle + 10_000]\n\n    # VARIABLE TO HOLD SPECTROGRAM\n    img = np.zeros((128, 256, 4), dtype=\"float32\")\n\n    if display:\n        plt.figure(figsize=(10, 7))\n    signals = []\n    for k in range(4):\n        COLS = FEATS[k]\n\n        for kk in range(4):\n\n            # COMPUTE PAIR DIFFERENCES\n            x = eeg[COLS[kk]].values - eeg[COLS[kk + 1]].values\n\n            # FILL NANS\n            m = np.nanmean(x)\n            if np.isnan(x).mean() < 1:\n                x = np.nan_to_num(x, nan=m)\n            else:\n                x[:] = 0\n\n            # DENOISE\n            if USE_WAVELET:\n                x = denoise(x, wavelet=USE_WAVELET)\n            signals.append(x)\n\n            # RAW SPECTROGRAM\n            mel_spec = librosa.feature.melspectrogram(\n                y=x,\n                sr=200,\n                hop_length=len(x) // 256,\n                n_fft=1024,\n                n_mels=128,\n                fmin=0,\n                fmax=20,\n                win_length=128,\n            )\n\n            # LOG TRANSFORM\n            width = (mel_spec.shape[1] // 32) * 32\n            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).astype(np.float32)[\n                :, :width\n            ]\n\n            # STANDARDIZE TO -1 TO 1\n            mel_spec_db = (mel_spec_db + 40) / 40\n            img[:, :, k] += mel_spec_db\n\n        # AVERAGE THE 4 MONTAGE DIFFERENCES\n        img[:, :, k] /= 4.0\n\n        if display:\n            plt.subplot(2, 2, k + 1)\n            plt.imshow(img[:, :, k], aspect=\"auto\", origin=\"lower\")\n            plt.title(f\"EEG {eeg_id} - Spectrogram {NAMES[k]}\")\n\n    if display:\n        plt.show()\n        plt.figure(figsize=(10, 5))\n        offset = 0\n        for k in range(4):\n            if k > 0:\n                offset -= signals[3 - k].min()\n            plt.plot(range(10_000), signals[k] + offset, label=NAMES[3 - k])\n            offset += signals[3 - k].max()\n        plt.legend()\n        plt.title(f\"EEG {eeg_id} Signals\")\n        plt.show()\n        print()\n        print(\"#\" * 25)\n        print()\n\n    return img","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.297058Z","iopub.status.idle":"2024-03-12T13:50:56.297397Z","shell.execute_reply.started":"2024-03-12T13:50:56.297227Z","shell.execute_reply":"2024-03-12T13:50:56.297243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE ALL EEG SPECTROGRAMS\nDISPLAY = 0\nEEG_IDS2 = test.eeg_id.unique()\nall_eegs2 = {}\n\nprint(\"Converting Test EEG to Spectrograms...\")\nprint()\nfor i, eeg_id in enumerate(EEG_IDS2):\n\n    # CREATE SPECTROGRAM FROM EEG PARQUET\n    all_eegs2[eeg_id] = spectrogram_from_eeg(\n        f\"{config.data_eeg_test}{eeg_id}.parquet\", False\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.298590Z","iopub.status.idle":"2024-03-12T13:50:56.298973Z","shell.execute_reply.started":"2024-03-12T13:50:56.298793Z","shell.execute_reply":"2024-03-12T13:50:56.298810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FEATURE NAMES\nSPEC_COLS = pd.read_parquet(f\"{config.data_spectograms}1000086677.parquet\").columns[1:]\nFEATURES = [f\"{c}_mean_10m\" for c in SPEC_COLS]\nFEATURES += [f\"{c}_min_10m\" for c in SPEC_COLS]\nFEATURES += [f\"{c}_mean_20s\" for c in SPEC_COLS]\nFEATURES += [f\"{c}_min_20s\" for c in SPEC_COLS]\nFEATURES += [f\"eeg_mean_f{x}_10s\" for x in range(512)]\nFEATURES += [f\"eeg_min_f{x}_10s\" for x in range(512)]\nFEATURES += [f\"eeg_max_f{x}_10s\" for x in range(512)]\nFEATURES += [f\"eeg_std_f{x}_10s\" for x in range(512)]\nprint(f\"We are creating {len(FEATURES)} features for {len(train)} rows... \", end=\"\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.300555Z","iopub.status.idle":"2024-03-12T13:50:56.301022Z","shell.execute_reply.started":"2024-03-12T13:50:56.300788Z","shell.execute_reply":"2024-03-12T13:50:56.300810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FEATURE ENGINEER TEST\ndata = np.zeros((len(test), len(FEATURES)))\n\nfor k in range(len(test)):\n    row = test.iloc[k]\n    s = int(row.spec_id)\n    spec = pd.read_parquet(f\"{config.data_spectograms_test}{s}.parquet\")\n\n    # 10 MINUTE WINDOW FEATURES\n    x = np.nanmean(spec.iloc[:, 1:].values, axis=0)\n    data[k, :400] = x\n    x = np.nanmin(spec.iloc[:, 1:].values, axis=0)\n    data[k, 400:800] = x\n\n    # 20 SECOND WINDOW FEATURES\n    x = np.nanmean(spec.iloc[145:155, 1:].values, axis=0)\n    data[k, 800:1200] = x\n    x = np.nanmin(spec.iloc[145:155, 1:].values, axis=0)\n    data[k, 1200:1600] = x\n\n    # RESHAPE EEG SPECTROGRAMS 128x256x4 => 512x256\n    eeg_spec = np.zeros((512, 256), dtype=\"float32\")\n    xx = all_eegs2[row.eeg_id]\n    for j in range(4):\n        eeg_spec[128 * j : 128 * (j + 1),] = xx[:, :, j]\n\n    # 10 SECOND WINDOW FROM EEG SPECTROGRAMS\n    x = np.nanmean(eeg_spec.T[100:-100, :], axis=0)\n    data[k, 1600:2112] = x\n    x = np.nanmin(eeg_spec.T[100:-100, :], axis=0)\n    data[k, 2112:2624] = x\n    x = np.nanmax(eeg_spec.T[100:-100, :], axis=0)\n    data[k, 2624:3136] = x\n    x = np.nanstd(eeg_spec.T[100:-100, :], axis=0)\n    data[k, 3136:3648] = x\n\ntest[FEATURES] = data\nprint(\"New test shape\", test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.302105Z","iopub.status.idle":"2024-03-12T13:50:56.302581Z","shell.execute_reply.started":"2024-03-12T13:50:56.302325Z","shell.execute_reply":"2024-03-12T13:50:56.302347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import catboost as cat\nfrom catboost import CatBoostClassifier, Pool","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.304053Z","iopub.status.idle":"2024-03-12T13:50:56.304365Z","shell.execute_reply.started":"2024-03-12T13:50:56.304209Z","shell.execute_reply":"2024-03-12T13:50:56.304224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INFER CATBOOST ON TEST\npreds = []\n\nfor i in range(5):\n    print(i, \", \", end=\"\")\n    model = CatBoostClassifier(task_type=\"GPU\")\n    model.load_model(\n        f\"{config.full_path}catboost-model/catboost_model/CAT_v{VER}_f{i}.cat\"\n    )\n\n    test_pool = Pool(data=test[FEATURES])\n\n    pred = model.predict_proba(test_pool)\n    preds.append(pred)\npred_model6 = np.mean(preds, axis=0)\nprint()\nprint(\"Test preds shape\", pred.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.305396Z","iopub.status.idle":"2024-03-12T13:50:56.305747Z","shell.execute_reply.started":"2024-03-12T13:50:56.305555Z","shell.execute_reply":"2024-03-12T13:50:56.305570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FREE MEMORY\ndel data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.307143Z","iopub.status.idle":"2024-03-12T13:50:56.307498Z","shell.execute_reply.started":"2024-03-12T13:50:56.307318Z","shell.execute_reply":"2024-03-12T13:50:56.307335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub6 = pd.DataFrame({\"eeg_id\": test.eeg_id.values})\nsub6[TARGETS] = pred_model6\nsub6.to_csv(\"submission.csv\", index=False)\nprint(\"Submissionn shape\", sub6.shape)\nsub6.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.309470Z","iopub.status.idle":"2024-03-12T13:50:56.309834Z","shell.execute_reply.started":"2024-03-12T13:50:56.309639Z","shell.execute_reply":"2024-03-12T13:50:56.309655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\nsub6.iloc[:, -6:].sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.320158Z","iopub.status.idle":"2024-03-12T13:50:56.320513Z","shell.execute_reply.started":"2024-03-12T13:50:56.320337Z","shell.execute_reply":"2024-03-12T13:50:56.320353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"import numpy as np\n# Scores for each model, where lower is better\nscores = np.array([0.43, 0.45, 0.41, 0.34, 0.46, 0.60])\n\ninverted_scores = 1 / scores\ntransformed_scores = inverted_scores ** 8\nweights = transformed_scores / transformed_scores.sum()\nweights","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.321716Z","iopub.status.idle":"2024-03-12T13:50:56.322169Z","shell.execute_reply.started":"2024-03-12T13:50:56.321981Z","shell.execute_reply":"2024-03-12T13:50:56.322001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_computed_weights = True\nmanual_weights = [0.05, 0.05, 0.25, 0.65, 0.00, 0.05]\nlabels = [\"seizure\", \"lpd\", \"gpd\", \"lrda\", \"grda\", \"other\"]\nsubmission = pd.read_csv(f\"{config.competition_data_path}/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.323275Z","iopub.status.idle":"2024-03-12T13:50:56.323626Z","shell.execute_reply.started":"2024-03-12T13:50:56.323453Z","shell.execute_reply":"2024-03-12T13:50:56.323470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(labels)):\n    if use_computed_weights:\n        # Use the computed weights\n        submission[f\"{labels[i]}_vote\"] = (\n            pred_model1[:, i] * weights[0] +\n            pred_model2[:, i] * weights[1] +\n            pred_model3[:, i] * weights[2] +\n            pred_model4[:, i] * weights[3] +\n            pred_model5[:, i] * weights[4] +\n            pred_model6[:, i] * weights[5]\n        )\n    else:\n        # Use the manually set weights\n        submission[f\"{labels[i]}_vote\"] = (\n            pred_model1[:, i] * manual_weights[0] +\n            pred_model2[:, i] * manual_weights[1] +\n            pred_model3[:, i] * manual_weights[2] +\n            pred_model4[:, i] * manual_weights[3] +\n            pred_model5[:, i] * manual_weights[4] +\n            pred_model6[:, i] * manual_weights[5]\n        )\nsubmission.to_csv(f\"{config.output_path}submission.csv\", index=None)\ndisplay(submission.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.324609Z","iopub.status.idle":"2024-03-12T13:50:56.324948Z","shell.execute_reply.started":"2024-03-12T13:50:56.324787Z","shell.execute_reply":"2024-03-12T13:50:56.324803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\n# submission.iloc[:, -6:].sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.326596Z","iopub.status.idle":"2024-03-12T13:50:56.326945Z","shell.execute_reply.started":"2024-03-12T13:50:56.326782Z","shell.execute_reply":"2024-03-12T13:50:56.326798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete this code\n# tempz=pd.read_csv(config.output_path)\n\n# tempz.iloc[0, 1] = 0.098192\n# tempz.iloc[0, 2] = 0.058772\n# tempz.iloc[0, 3] = 0.000671\n# tempz.iloc[0, 4] = 0.395016\n# tempz.iloc[0, 5] = 0.023945\n# tempz.iloc[0, 6] = 0.423405\n\n# tempz.to_csv(\"/kaggle/working/submission.csv\", index=False)\n# display(tempz.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.327955Z","iopub.status.idle":"2024-03-12T13:50:56.328300Z","shell.execute_reply.started":"2024-03-12T13:50:56.328129Z","shell.execute_reply":"2024-03-12T13:50:56.328146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n\n# Read the CSV file\nfinalrounding = pd.read_csv(f\"{config.output_path}submission.csv\")\n\nseizure_vote_value = finalrounding.iloc[0, 1]\n\nlpd_vote_value = finalrounding.iloc[0, 2]\n\ngpd_vote_value = finalrounding.iloc[0, 3]\n\nlrda_vote_value = finalrounding.iloc[0, 4]\n\ngrda_vote_value = finalrounding.iloc[0, 5]\n\nother_vote_value = finalrounding.iloc[0, 6]\n\nbuffer_value = 0.0\n\nif seizure_vote_value < 0.06:\n    buffer_value = buffer_value + seizure_vote_value\n    finalrounding.iloc[0, 1] = 0\n    seizure_vote_value = 0\n\nif lpd_vote_value < 0.06:\n    buffer_value = buffer_value + lpd_vote_value\n    finalrounding.iloc[0, 2] = 0\n    lpd_vote_value = 0\n\nif gpd_vote_value < 0.06:\n    buffer_value = buffer_value + gpd_vote_value\n    finalrounding.iloc[0, 3] = 0\n    gpd_vote_value = 0\n\nif lrda_vote_value < 0.06:\n    buffer_value = buffer_value + lrda_vote_value\n    finalrounding.iloc[0, 4] = 0\n    lrda_vote_value = 0\n\nif grda_vote_value < 0.06:\n    buffer_value = buffer_value + grda_vote_value\n    finalrounding.iloc[0, 5] = 0\n    grda_vote_value = 0\n\nif other_vote_value < 0.06:\n    buffer_value = buffer_value + other_vote_value\n    finalrounding.iloc[0, 6] = 0\n    other_vote_value = 0\n\nvote_dict = {\n    \"var1\": seizure_vote_value,\n    \"var2\": lpd_vote_value,\n    \"var3\": gpd_vote_value,\n    \"var4\": lrda_vote_value,\n    \"var5\": grda_vote_value,\n}\n\nbiggest_var_name = max(vote_dict, key=vote_dict.get)\nbiggest_value = vote_dict[biggest_var_name]\n\nif biggest_var_name == \"var1\":\n    finalrounding.iloc[0, 1] += buffer_value\n\nif biggest_var_name == \"var2\":\n    finalrounding.iloc[0, 2] += buffer_value\n\nif biggest_var_name == \"var3\":\n    finalrounding.iloc[0, 3] += buffer_value\n\nif biggest_var_name == \"var4\":\n    finalrounding.iloc[0, 4] += buffer_value\n\nif biggest_var_name == \"var5\":\n    finalrounding.iloc[0, 5] += buffer_value\n\nfinalrounding.to_csv((f\"{config.output_path}submission.csv\"), index=False)\nfinalrounding.iloc[:, -6:].sum(axis=1)\ndisplay(finalrounding.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.329464Z","iopub.status.idle":"2024-03-12T13:50:56.329826Z","shell.execute_reply.started":"2024-03-12T13:50:56.329626Z","shell.execute_reply":"2024-03-12T13:50:56.329642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\n# finalrounding.iloc[:, -6:].sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T13:50:56.330748Z","iopub.status.idle":"2024-03-12T13:50:56.331069Z","shell.execute_reply.started":"2024-03-12T13:50:56.330908Z","shell.execute_reply":"2024-03-12T13:50:56.330923Z"},"trusted":true},"execution_count":null,"outputs":[]}]}